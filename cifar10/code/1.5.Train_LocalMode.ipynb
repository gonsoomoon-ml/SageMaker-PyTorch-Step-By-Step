{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1.5] 로컬 모드 훈련\n",
    "\n",
    "본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 아래는 세이지메이커의 어떤 피쳐도 사용하지 않고, PyTorch 만을 사용해서 훈련 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch CIFAR-10 local training  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 inputs:  s3://sagemaker-ap-northeast-2-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"../data\", bucket=bucket, key_prefix=\"data/cifar10\")\n",
    "print(\"s3 inputs: \", inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training \n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 3rbonk6gu9-algo-1-oxgd4 ... \n",
      "Creating 3rbonk6gu9-algo-1-oxgd4 ... done\n",
      "Attaching to 3rbonk6gu9-algo-1-oxgd4\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:42:45,709 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:42:45,753 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:42:45,756 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:42:45,912 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Collecting torchsummary==1.5.1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Collecting sagemaker_inference==1.5.5\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m   Downloading sagemaker_inference-1.5.5.tar.gz (20 kB)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.19.1)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.15.0)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (5.8.0)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: retrying==1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.3.3)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.5.2)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Building wheels for collected packages: sagemaker-inference\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m   Building wheel for sagemaker-inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \u001b[?25h  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.5.5-py2.py3-none-any.whl size=26977 sha256=a60e12f223ec5d480ed5129fc2469eb404402c0ad989f9f09678f59b9cf4c47b\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/a4/bf/81/8e084e445a44e9fbc9d64efc7afb2a660ecd06285ea4a51fa0\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Successfully built sagemaker-inference\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Installing collected packages: torchsummary, sagemaker-inference\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Successfully installed sagemaker-inference-1.5.5 torchsummary-1.5.1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:42:49,020 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Training Env:\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     },\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"current_host\": \"algo-1-oxgd4\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"algo-1-oxgd4\"\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     ],\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"lr\": 0.1,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"batch_size\": 16\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     },\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"training\": {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         }\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     },\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"job_name\": \"pytorch-training-2021-06-07-13-42-38-627\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"master_hostname\": \"algo-1-oxgd4\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-42-38-627/source/sourcedir.tar.gz\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"num_gpus\": 4,\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"current_host\": \"algo-1-oxgd4\",\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m             \"algo-1-oxgd4\"\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m         ]\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     },\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m }\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Environment variables:\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_HOSTS=[\"algo-1-oxgd4\"]\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_HPS={\"batch_size\":16,\"epochs\":1,\"lr\":0.1}\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-oxgd4\",\"hosts\":[\"algo-1-oxgd4\"]}\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_CURRENT_HOST=algo-1-oxgd4\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_NUM_GPUS=4\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-42-38-627/source/sourcedir.tar.gz\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-oxgd4\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-oxgd4\"],\"hyperparameters\":{\"batch_size\":16,\"epochs\":1,\"lr\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-06-07-13-42-38-627\",\"log_level\":20,\"master_hostname\":\"algo-1-oxgd4\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-42-38-627/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-oxgd4\",\"hosts\":[\"algo-1-oxgd4\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"16\",\"--epochs\",\"1\",\"--lr\",\"0.1\"]\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_HP_LR=0.1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m SM_HP_BATCH_SIZE=16\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m /opt/conda/bin/python3.6 train.py --batch_size 16 --epochs 1 --lr 0.1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Distributed training - False\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Device Type: cuda\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Loading Cifar10 dataset\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Model loaded from get_model_network()\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m [2021-06-07 13:42:52.551 algo-1-oxgd4:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m [2021-06-07 13:42:52.791 algo-1-oxgd4:37 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m NCCL version 2.4.8+cuda10.1\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m [1,  2000] loss: 2.330\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Finished Training\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m Saving the model.\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m \n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 |\u001b[0m 2021-06-07 13:43:37,639 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36m3rbonk6gu9-algo-1-oxgd4 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "\n",
    "local_cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='source',    \n",
    "    role=role,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    session = sagemaker_session,\n",
    "    hyperparameters={'epochs': 1, \n",
    "                     'lr': 0.1,\n",
    "                     'batch_size': 16\n",
    "                    }                      \n",
    "    \n",
    ")\n",
    "local_cifar10_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Host Mode 로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-07 13:44:58 Starting - Starting the training job...\n",
      "2021-06-07 13:45:24 Starting - Launching requested ML instancesProfilerReport-1623073498: InProgress\n",
      "......\n",
      "2021-06-07 13:46:24 Starting - Preparing the instances for training.........\n",
      "2021-06-07 13:47:49 Downloading - Downloading input data......\n",
      "2021-06-07 13:48:44 Training - Downloading the training image......\n",
      "2021-06-07 13:49:47 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-07 13:49:48,830 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-07 13:49:48,854 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-07 13:49:48,863 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-07 13:49:49,241 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[34mCollecting torchsummary==1.5.1\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker_inference==1.5.5\n",
      "  Downloading sagemaker_inference-1.5.5.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (5.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: retrying==1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker-inference\n",
      "  Building wheel for sagemaker-inference (setup.py): started\n",
      "  Building wheel for sagemaker-inference (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.5.5-py2.py3-none-any.whl size=26977 sha256=7ca88765397e7543773e0f1c3c2092287ee5e6a188d23c43f015ba0c2e328a4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/bf/81/8e084e445a44e9fbc9d64efc7afb2a660ecd06285ea4a51fa0\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker-inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torchsummary, sagemaker-inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-inference-1.5.5 torchsummary-1.5.1\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 13:49:52,879 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"lr\": 0.01,\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-06-07-13-44-58-337\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-44-58-337/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":16,\"epochs\":2,\"lr\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-44-58-337/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":16,\"epochs\":2,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-06-07-13-44-58-337\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-44-58-337/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"16\",\"--epochs\",\"2\",\"--lr\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 16 --epochs 2 --lr 0.01\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoading Cifar10 dataset\u001b[0m\n",
      "\u001b[34mModel loaded from get_model_network()\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.516 algo-1:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.824 algo-1:37 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.824 algo-1:37 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.825 algo-1:37 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.825 algo-1:37 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.825 algo-1:37 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.conv1.weight count_params:450\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.conv1.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.conv2.weight count_params:2400\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.conv2.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.fc1.weight count_params:48000\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.fc1.bias count_params:120\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.826 algo-1:37 INFO hook.py:584] name:module.fc2.weight count_params:10080\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.827 algo-1:37 INFO hook.py:584] name:module.fc2.bias count_params:84\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.827 algo-1:37 INFO hook.py:584] name:module.fc3.weight count_params:840\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.827 algo-1:37 INFO hook.py:584] name:module.fc3.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.827 algo-1:37 INFO hook.py:586] Total Trainable Params: 62006\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.827 algo-1:37 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-06-07 13:49:59.829 algo-1:37 INFO hook.py:476] Hook is writing from the hook with pid: 37\n",
      "\u001b[0m\n",
      "\u001b[34m[1,  2000] loss: 2.064\u001b[0m\n",
      "\u001b[34m[2,  2000] loss: 1.889\u001b[0m\n",
      "\u001b[34mFinished Training\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2021-06-07 13:50:39,324 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-06-07 13:50:54 Uploading - Uploading generated training model\n",
      "2021-06-07 13:50:54 Completed - Training job completed\n",
      "Training seconds: 185\n",
      "Billable seconds: 185\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='source',    \n",
    "    role=role,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    hyperparameters={'epochs': 2, \n",
    "                     'lr': 0.01,\n",
    "                     'batch_size': 16                     \n",
    "                    }                      \n",
    "    \n",
    ")\n",
    "cifar10_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 아티펙트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact_path:  s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-44-58-337/output/model.tar.gz\n",
      "Stored 'artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "artifact_path = cifar10_estimator.model_data\n",
    "print(\"artifact_path: \", artifact_path)\n",
    "\n",
    "%store artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_artifact_path:  s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2021-06-07-13-42-38-627/model.tar.gz\n",
      "Stored 'local_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "local_artifact_path = local_cifar10_estimator.model_data\n",
    "print(\"local_artifact_path: \", local_artifact_path)\n",
    "%store local_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 ls {local_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
