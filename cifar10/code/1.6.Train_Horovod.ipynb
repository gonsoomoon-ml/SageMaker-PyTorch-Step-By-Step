{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1.6] 호로보드로 분산 훈련 (로컬 모드 및 호스트 모드)\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 준비 작업을 걸쳐서 현재의 노트북 인스턴스에서 로컬 모드로 호로보드로 모델 훈련\n",
    "- 호스트 모드에서 2개의 인스턴스로 호로보드 모델 훈련\n",
    "- 훈련된 모델 아티펙트 저장\n",
    "\n",
    "## 참고:\n",
    "- [파이토치 호로보드 공식 예시](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_horovod_mnist)\n",
    "- 세이지 메이커로 파이토치 사용 --> [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 inputs:  s3://sagemaker-us-east-1-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"../data\", bucket=bucket, key_prefix=\"data/cifar10\")\n",
    "print(\"s3 inputs: \", inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "# instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "job_name ='cifar10-horovod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시스템의 이전 도커 컨테이너 삭제\n",
    "- 아래와 같은 명령어를 사용하여 저장 공간을 확보 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 컨테이너 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G  320K  241G   1% /dev/shm\n",
      "/dev/xvda1      109G   91G   18G  85% /\n",
      "/dev/xvdf       984G   23G  911G   3% /home/ec2-user/SageMaker\n",
      "unknown flag: --all\n",
      "See 'docker container prune --help'.\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G  320K  241G   1% /dev/shm\n",
      "/dev/xvda1      109G   91G   18G  85% /\n",
      "/dev/xvdf       984G   23G  911G   3% /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "! df -h\n",
    "! docker container prune -f --all\n",
    "! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 이미지 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G  320K  241G   1% /dev/shm\n",
      "/dev/xvda1      109G   91G   18G  85% /\n",
      "/dev/xvdf       984G   23G  911G   3% /home/ec2-user/SageMaker\n",
      "Total reclaimed space: 0B\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G  320K  241G   1% /dev/shm\n",
      "/dev/xvda1      109G   91G   18G  85% /\n",
      "/dev/xvdf       984G   23G  911G   3% /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "! df -h\n",
    "! docker image prune -f --all\n",
    "! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 로컬모드로 훈련 \n",
    "- 현 실행 노트북 인스턴스에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating w7brcs2cp0-algo-1-y7hwy ... \n",
      "Creating w7brcs2cp0-algo-1-y7hwy ... done\n",
      "Attaching to w7brcs2cp0-algo-1-y7hwy\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:26:40,048 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:26:40,129 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:26:40,132 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:26:40,298 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Requirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Collecting torchsummary==1.5.1\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Installing collected packages: torchsummary\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Successfully installed torchsummary-1.5.1\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:26:42,156 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Training Env:\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     },\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"current_host\": \"algo-1-y7hwy\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"algo-1-y7hwy\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     ],\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"epochs\": 5,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"lr\": 0.001,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"batch-size\": 64,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"log-interval\": 100,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"backend\": \"gloo\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     },\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"training\": {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         }\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     },\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"job_name\": \"cifar10-horovod-2021-09-27-11-26-31-356\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"master_hostname\": \"algo-1-y7hwy\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-26-31-356/source/sourcedir.tar.gz\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"module_name\": \"train_horovod\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"current_host\": \"algo-1-y7hwy\",\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m             \"algo-1-y7hwy\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m         ]\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     },\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m     \"user_entry_point\": \"train_horovod.py\"\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m }\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Environment variables:\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HOSTS=[\"algo-1-y7hwy\"]\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":5,\"log-interval\":100,\"lr\":0.001}\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_USER_ENTRY_POINT=train_horovod.py\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-y7hwy\",\"hosts\":[\"algo-1-y7hwy\"]}\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_CURRENT_HOST=algo-1-y7hwy\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_MODULE_NAME=train_horovod\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-26-31-356/source/sourcedir.tar.gz\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-y7hwy\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-y7hwy\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":5,\"log-interval\":100,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-horovod-2021-09-27-11-26-31-356\",\"log_level\":20,\"master_hostname\":\"algo-1-y7hwy\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-26-31-356/source/sourcedir.tar.gz\",\"module_name\":\"train_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-y7hwy\",\"hosts\":[\"algo-1-y7hwy\"]},\"user_entry_point\":\"train_horovod.py\"}\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"5\",\"--log-interval\",\"100\",\"--lr\",\"0.001\"]\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HP_EPOCHS=5\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HP_LR=0.001\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HP_BATCH-SIZE=64\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HP_LOG-INTERVAL=100\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m SM_HP_BACKEND=gloo\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m /opt/conda/bin/python3.6 train_horovod.py --backend gloo --batch-size 64 --epochs 5 --log-interval 100 --lr 0.001\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m args: Namespace(backend='gloo', batch_size=64, current_host='algo-1-y7hwy', data_dir='/opt/ml/input/data/training', epochs=5, hosts=['algo-1-y7hwy'], log_interval=100, lr=0.001, model_dir='/opt/ml/model', momentum=0.5, num_gpus=8, seed=42, test_batch_size=1000, workers=2)\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Get train data sampler and data loader\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Get test data sampler and data loader\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Processes 50000/50000 (100%) of train data\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Processes 10000/10000 (100%) of test data\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Model loaded\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m [2021-09-27 11:26:46.129 algo-1-y7hwy:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m [2021-09-27 11:26:46.372 algo-1-y7hwy:32 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m NCCL version 2.4.8+cuda10.1\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [6400/50000 (13%)] Loss: 2.302\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [12800/50000 (26%)] Loss: 2.286\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [19200/50000 (38%)] Loss: 2.295\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [25600/50000 (51%)] Loss: 2.299\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [32000/50000 (64%)] Loss: 2.300\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [38400/50000 (77%)] Loss: 2.302\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 1 [44800/50000 (90%)] Loss: 2.294\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Test set: Average loss: -0.0015, Accuracy: 11.44%\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [6400/50000 (13%)] Loss: 2.299\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [12800/50000 (26%)] Loss: 2.288\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [19200/50000 (38%)] Loss: 2.294\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [25600/50000 (51%)] Loss: 2.298\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [32000/50000 (64%)] Loss: 2.297\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [38400/50000 (77%)] Loss: 2.300\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 2 [44800/50000 (90%)] Loss: 2.293\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Test set: Average loss: -0.0016, Accuracy: 11.31%\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [6400/50000 (13%)] Loss: 2.296\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [12800/50000 (26%)] Loss: 2.290\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [19200/50000 (38%)] Loss: 2.293\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [25600/50000 (51%)] Loss: 2.297\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [32000/50000 (64%)] Loss: 2.293\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [38400/50000 (77%)] Loss: 2.297\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 3 [44800/50000 (90%)] Loss: 2.290\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Test set: Average loss: -0.0033, Accuracy: 16.23%\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [6400/50000 (13%)] Loss: 2.289\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [12800/50000 (26%)] Loss: 2.290\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [19200/50000 (38%)] Loss: 2.289\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [25600/50000 (51%)] Loss: 2.294\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [32000/50000 (64%)] Loss: 2.283\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [38400/50000 (77%)] Loss: 2.290\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 4 [44800/50000 (90%)] Loss: 2.283\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Test set: Average loss: -0.0094, Accuracy: 15.32%\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [6400/50000 (13%)] Loss: 2.274\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [12800/50000 (26%)] Loss: 2.288\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [19200/50000 (38%)] Loss: 2.277\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [25600/50000 (51%)] Loss: 2.284\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [32000/50000 (64%)] Loss: 2.254\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [38400/50000 (77%)] Loss: 2.272\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Train Epoch: 5 [44800/50000 (90%)] Loss: 2.258\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Test set: Average loss: -0.0366, Accuracy: 13.91%\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Training is finished\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m Saving the model.\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m /opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m \n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy |\u001b[0m 2021-09-27 11:28:37,109 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mw7brcs2cp0-algo-1-y7hwy exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train_horovod.py\",    \n",
    "    source_dir='source',    \n",
    "    base_job_name = job_name,\n",
    "    role=role,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=instance_type,\n",
    "    hyperparameters={\"epochs\": 5, \n",
    "                     'lr': 0.001,\n",
    "                     'batch-size': 64,\n",
    "                     'log-interval' : 100,\n",
    "                     \"backend\": \"gloo\",                     \n",
    "                    },    \n",
    ")\n",
    "cifar10_estimator.fit({\"training\" : inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬모드에서 도커 이미지 다운로드 된 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                      TAG                 IMAGE ID            CREATED             SIZE\n",
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training   1.6.0-gpu-py3       30e42e4701a4        5 months ago        8.6GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 호스트 모드로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 11:32:22 Starting - Starting the training job...\n",
      "2021-09-27 11:32:46 Starting - Launching requested ML instancesProfilerReport-1632742342: InProgress\n",
      ".........\n",
      "2021-09-27 11:34:07 Starting - Preparing the instances for training.........\n",
      "2021-09-27 11:35:47 Downloading - Downloading input data...\n",
      "2021-09-27 11:36:07 Training - Downloading the training image.....\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-09-27 11:37:06,790 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-09-27 11:37:06,833 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-09-27 11:37:08,257 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-09-27 11:37:08,666 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[35mCollecting torchsummary==1.5.1\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: torchsummary\u001b[0m\n",
      "\u001b[35mSuccessfully installed torchsummary-1.5.1\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-27 11:37:10,820 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"lr\": 0.001,\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"cifar10-horovod-2021-09-27-11-32-22-045\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-32-22-045/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_horovod\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_horovod.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":10,\"lr\":0.001}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train_horovod.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train_horovod\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-32-22-045/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":10,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"cifar10-horovod-2021-09-27-11-32-22-045\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-32-22-045/source/sourcedir.tar.gz\",\"module_name\":\"train_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_horovod.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"10\",\"--lr\",\"0.001\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[35mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 train_horovod.py --backend gloo --batch-size 64 --epochs 10 --lr 0.001\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35margs: Namespace(backend='gloo', batch_size=64, current_host='algo-2', data_dir='/opt/ml/input/data/training', epochs=10, hosts=['algo-1', 'algo-2'], log_interval=10, lr=0.001, model_dir='/opt/ml/model', momentum=0.5, num_gpus=4, seed=42, test_batch_size=1000, workers=2)\u001b[0m\n",
      "\u001b[35mGet train data sampler and data loader\u001b[0m\n",
      "\n",
      "2021-09-27 11:37:07 Training - Training image download completed. Training in progress.\u001b[35mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [640/50000 (1%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1280/50000 (3%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [1920/50000 (4%)] Loss: 2.322\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [2560/50000 (5%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3200/50000 (6%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [3840/50000 (8%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [4480/50000 (9%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5120/50000 (10%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [5760/50000 (12%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/50000 (13%)] Loss: 2.331\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7040/50000 (14%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [7680/50000 (15%)] Loss: 2.286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8320/50000 (17%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [8960/50000 (18%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [9600/50000 (19%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10240/50000 (20%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [10880/50000 (22%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [11520/50000 (23%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12160/50000 (24%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/50000 (26%)] Loss: 2.311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [640/50000 (1%)] Loss: 2.310\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [13440/50000 (27%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14080/50000 (28%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [14720/50000 (29%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [15360/50000 (31%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16000/50000 (32%)] Loss: 2.321\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [16640/50000 (33%)] Loss: 2.309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/50000 (3%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1920/50000 (4%)] Loss: 2.310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/50000 (5%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/50000 (6%)] Loss: 2.309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/50000 (8%)] Loss: 2.318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4480/50000 (9%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17280/50000 (35%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [17920/50000 (36%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [18560/50000 (37%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/50000 (38%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19840/50000 (40%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/50000 (10%)] Loss: 2.306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5760/50000 (12%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/50000 (13%)] Loss: 2.315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7040/50000 (14%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/50000 (15%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8320/50000 (17%)] Loss: 2.311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [20480/50000 (41%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21120/50000 (42%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [21760/50000 (43%)] Loss: 2.313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [22400/50000 (45%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23040/50000 (46%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [23680/50000 (47%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/50000 (18%)] Loss: 2.317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [9600/50000 (19%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/50000 (20%)] Loss: 2.316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10880/50000 (22%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/50000 (23%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24320/50000 (49%)] Loss: 2.311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [24960/50000 (50%)] Loss: 2.313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/50000 (51%)] Loss: 2.311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26240/50000 (52%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [26880/50000 (54%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [27520/50000 (55%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12160/50000 (24%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/50000 (26%)] Loss: 2.310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [13440/50000 (27%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/50000 (28%)] Loss: 2.308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14720/50000 (29%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/50000 (31%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28160/50000 (56%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [28800/50000 (58%)] Loss: 2.310\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [29440/50000 (59%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30080/50000 (60%)] Loss: 2.316\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [30720/50000 (61%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [31360/50000 (63%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32000/50000 (64%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [32640/50000 (65%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33280/50000 (66%)] Loss: 2.315\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [33920/50000 (68%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [34560/50000 (69%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35200/50000 (70%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [35840/50000 (72%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [36480/50000 (73%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37120/50000 (74%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [37760/50000 (75%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/50000 (77%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39040/50000 (78%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [39680/50000 (79%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40320/50000 (81%)] Loss: 2.317\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [40960/50000 (82%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [41600/50000 (83%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42240/50000 (84%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [42880/50000 (86%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [43520/50000 (87%)] Loss: 2.310\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44160/50000 (88%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [44800/50000 (90%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [45440/50000 (91%)] Loss: 2.316\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46080/50000 (92%)] Loss: 2.311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [46720/50000 (93%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [47360/50000 (95%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48000/50000 (96%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [48640/50000 (97%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49280/50000 (98%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [49920/50000 (100%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0018, Accuracy: 10.81%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [640/50000 (1%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1280/50000 (3%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [1920/50000 (4%)] Loss: 2.317\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [2560/50000 (5%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3200/50000 (6%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [3840/50000 (8%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [4480/50000 (9%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5120/50000 (10%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [5760/50000 (12%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/50000 (13%)] Loss: 2.323\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7040/50000 (14%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [7680/50000 (15%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8320/50000 (17%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [8960/50000 (18%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [9600/50000 (19%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10240/50000 (20%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [10880/50000 (22%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [11520/50000 (23%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12160/50000 (24%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/50000 (26%)] Loss: 2.308\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0158, Accuracy: 14.20%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [640/50000 (1%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1280/50000 (3%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1920/50000 (4%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [13440/50000 (27%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14080/50000 (28%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [14720/50000 (29%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [15360/50000 (31%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16000/50000 (32%)] Loss: 2.315\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [16640/50000 (33%)] Loss: 2.306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2560/50000 (5%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/50000 (6%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3840/50000 (8%)] Loss: 2.313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [4480/50000 (9%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5120/50000 (10%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17280/50000 (35%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [17920/50000 (36%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [18560/50000 (37%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/50000 (38%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19840/50000 (40%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [20480/50000 (41%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5760/50000 (12%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/50000 (13%)] Loss: 2.311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7040/50000 (14%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7680/50000 (15%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8320/50000 (17%)] Loss: 2.308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8960/50000 (18%)] Loss: 2.313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21120/50000 (42%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [21760/50000 (43%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [22400/50000 (45%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23040/50000 (46%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [23680/50000 (47%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [9600/50000 (19%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/50000 (20%)] Loss: 2.311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10880/50000 (22%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [11520/50000 (23%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12160/50000 (24%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/50000 (26%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24320/50000 (49%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [24960/50000 (50%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/50000 (51%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26240/50000 (52%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [26880/50000 (54%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [27520/50000 (55%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [13440/50000 (27%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14080/50000 (28%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14720/50000 (29%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [15360/50000 (31%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16000/50000 (32%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28160/50000 (56%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [28800/50000 (58%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [29440/50000 (59%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30080/50000 (60%)] Loss: 2.312\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [30720/50000 (61%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [31360/50000 (63%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16640/50000 (33%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17280/50000 (35%)] Loss: 2.310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17920/50000 (36%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [18560/50000 (37%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/50000 (38%)] Loss: 2.308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19840/50000 (40%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32000/50000 (64%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [32640/50000 (65%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33280/50000 (66%)] Loss: 2.311\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [33920/50000 (68%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [34560/50000 (69%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/50000 (41%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21120/50000 (42%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21760/50000 (43%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [22400/50000 (45%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23040/50000 (46%)] Loss: 2.306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23680/50000 (47%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35200/50000 (70%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [35840/50000 (72%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [36480/50000 (73%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37120/50000 (74%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [37760/50000 (75%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/50000 (77%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24320/50000 (49%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24960/50000 (50%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/50000 (51%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26240/50000 (52%)] Loss: 2.306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26880/50000 (54%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39040/50000 (78%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [39680/50000 (79%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40320/50000 (81%)] Loss: 2.313\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [40960/50000 (82%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [41600/50000 (83%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [27520/50000 (55%)] Loss: 2.309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28160/50000 (56%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28800/50000 (58%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [29440/50000 (59%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30080/50000 (60%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/50000 (61%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42240/50000 (84%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [42880/50000 (86%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [43520/50000 (87%)] Loss: 2.307\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44160/50000 (88%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [44800/50000 (90%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [45440/50000 (91%)] Loss: 2.312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [31360/50000 (63%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/50000 (64%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32640/50000 (65%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33280/50000 (66%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33920/50000 (68%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [34560/50000 (69%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46080/50000 (92%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [46720/50000 (93%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [47360/50000 (95%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48000/50000 (96%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [48640/50000 (97%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49280/50000 (98%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [49920/50000 (100%)] Loss: 2.306\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0138, Accuracy: 14.65%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [640/50000 (1%)] Loss: 2.304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1280/50000 (3%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1920/50000 (4%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2560/50000 (5%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [13440/50000 (27%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14080/50000 (28%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [14720/50000 (29%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [15360/50000 (31%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16000/50000 (32%)] Loss: 2.310\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [16640/50000 (33%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3200/50000 (6%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3840/50000 (8%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [4480/50000 (9%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5120/50000 (10%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [5760/50000 (12%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17280/50000 (35%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [17920/50000 (36%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [18560/50000 (37%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/50000 (38%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19840/50000 (40%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [20480/50000 (41%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/50000 (13%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7040/50000 (14%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [7680/50000 (15%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8320/50000 (17%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [8960/50000 (18%)] Loss: 2.309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [9600/50000 (19%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21120/50000 (42%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [21760/50000 (43%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [22400/50000 (45%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23040/50000 (46%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [23680/50000 (47%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/50000 (20%)] Loss: 2.307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10880/50000 (22%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [11520/50000 (23%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12160/50000 (24%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/50000 (26%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24320/50000 (49%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [24960/50000 (50%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/50000 (51%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26240/50000 (52%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [26880/50000 (54%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [27520/50000 (55%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [13440/50000 (27%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14080/50000 (28%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [14720/50000 (29%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [15360/50000 (31%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [16000/50000 (32%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28160/50000 (56%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [28800/50000 (58%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [29440/50000 (59%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30080/50000 (60%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [30720/50000 (61%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [31360/50000 (63%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32000/50000 (64%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [32640/50000 (65%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33280/50000 (66%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [33920/50000 (68%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [34560/50000 (69%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35200/50000 (70%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [35840/50000 (72%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [36480/50000 (73%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37120/50000 (74%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [37760/50000 (75%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/50000 (77%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39040/50000 (78%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [39680/50000 (79%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40320/50000 (81%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [40960/50000 (82%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [41600/50000 (83%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42240/50000 (84%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [42880/50000 (86%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [43520/50000 (87%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44160/50000 (88%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [44800/50000 (90%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [45440/50000 (91%)] Loss: 2.309\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46080/50000 (92%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [46720/50000 (93%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [47360/50000 (95%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48000/50000 (96%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [48640/50000 (97%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [33920/50000 (68%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [34560/50000 (69%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35200/50000 (70%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [35840/50000 (72%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [36480/50000 (73%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37120/50000 (74%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49280/50000 (98%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [49920/50000 (100%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [37760/50000 (75%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/50000 (77%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39040/50000 (78%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [39680/50000 (79%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40320/50000 (81%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/50000 (82%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [41600/50000 (83%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42240/50000 (84%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [42880/50000 (86%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [43520/50000 (87%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44160/50000 (88%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/50000 (90%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [45440/50000 (91%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46080/50000 (92%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [46720/50000 (93%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [47360/50000 (95%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0011, Accuracy: 11.26%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [640/50000 (1%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [1280/50000 (3%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48000/50000 (96%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [48640/50000 (97%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49280/50000 (98%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [49920/50000 (100%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [1920/50000 (4%)] Loss: 2.308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [2560/50000 (5%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [3200/50000 (6%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [3840/50000 (8%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [4480/50000 (9%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [5120/50000 (10%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [5760/50000 (12%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/50000 (13%)] Loss: 2.312\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [7040/50000 (14%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [7680/50000 (15%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [8320/50000 (17%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [8960/50000 (18%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [9600/50000 (19%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [10240/50000 (20%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [10880/50000 (22%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [11520/50000 (23%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12160/50000 (24%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0110, Accuracy: 14.73%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [640/50000 (1%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1280/50000 (3%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/50000 (26%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [13440/50000 (27%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [14080/50000 (28%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [14720/50000 (29%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [15360/50000 (31%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [16000/50000 (32%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1920/50000 (4%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [2560/50000 (5%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [3200/50000 (6%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [3840/50000 (8%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [4480/50000 (9%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [16640/50000 (33%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [17280/50000 (35%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [17920/50000 (36%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [18560/50000 (37%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/50000 (38%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [5120/50000 (10%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [5760/50000 (12%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/50000 (13%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [7040/50000 (14%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [7680/50000 (15%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19840/50000 (40%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [20480/50000 (41%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [21120/50000 (42%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [21760/50000 (43%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [22400/50000 (45%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [23040/50000 (46%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [8320/50000 (17%)] Loss: 2.302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [8960/50000 (18%)] Loss: 2.305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [9600/50000 (19%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10240/50000 (20%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10880/50000 (22%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [11520/50000 (23%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12160/50000 (24%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/50000 (26%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [13440/50000 (27%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [14080/50000 (28%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [14720/50000 (29%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [15360/50000 (31%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [16000/50000 (32%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [16640/50000 (33%)] Loss: 2.284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [17280/50000 (35%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [17920/50000 (36%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [18560/50000 (37%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/50000 (38%)] Loss: 2.301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19840/50000 (40%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [20480/50000 (41%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [21120/50000 (42%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [21760/50000 (43%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [22400/50000 (45%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [23040/50000 (46%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [23680/50000 (47%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [24320/50000 (49%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [24960/50000 (50%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/50000 (51%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [26240/50000 (52%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [26880/50000 (54%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [27520/50000 (55%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [28160/50000 (56%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [28800/50000 (58%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [41600/50000 (83%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [42240/50000 (84%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [42880/50000 (86%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [43520/50000 (87%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [44160/50000 (88%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [44800/50000 (90%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [29440/50000 (59%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30080/50000 (60%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30720/50000 (61%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [31360/50000 (63%)] Loss: 2.286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/50000 (64%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [45440/50000 (91%)] Loss: 2.305\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [46080/50000 (92%)] Loss: 2.303\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [46720/50000 (93%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [47360/50000 (95%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [48000/50000 (96%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32640/50000 (65%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [33280/50000 (66%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [33920/50000 (68%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [34560/50000 (69%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [35200/50000 (70%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [35840/50000 (72%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [48640/50000 (97%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [49280/50000 (98%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [49920/50000 (100%)] Loss: 2.303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [36480/50000 (73%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [37120/50000 (74%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [37760/50000 (75%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/50000 (77%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [39040/50000 (78%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [39680/50000 (79%)] Loss: 2.286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40320/50000 (81%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40960/50000 (82%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [41600/50000 (83%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [42240/50000 (84%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [42880/50000 (86%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [43520/50000 (87%)] Loss: 2.284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44160/50000 (88%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/50000 (90%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [45440/50000 (91%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [46080/50000 (92%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [46720/50000 (93%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTest set: Average loss: -0.0009, Accuracy: 11.23%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [640/50000 (1%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [1280/50000 (3%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [1920/50000 (4%)] Loss: 2.304\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [2560/50000 (5%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [47360/50000 (95%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [48000/50000 (96%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [48640/50000 (97%)] Loss: 2.276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [49280/50000 (98%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [49920/50000 (100%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [3200/50000 (6%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [3840/50000 (8%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [4480/50000 (9%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [5120/50000 (10%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [5760/50000 (12%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/50000 (13%)] Loss: 2.306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [7040/50000 (14%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [7680/50000 (15%)] Loss: 2.290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [8320/50000 (17%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [8960/50000 (18%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [9600/50000 (19%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [10240/50000 (20%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [10880/50000 (22%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [11520/50000 (23%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12160/50000 (24%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/50000 (26%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [13440/50000 (27%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0059, Accuracy: 14.73%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [640/50000 (1%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [14080/50000 (28%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [14720/50000 (29%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [15360/50000 (31%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [16000/50000 (32%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [16640/50000 (33%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [1280/50000 (3%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [1920/50000 (4%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [2560/50000 (5%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [3200/50000 (6%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [3840/50000 (8%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [4480/50000 (9%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [17280/50000 (35%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [17920/50000 (36%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [18560/50000 (37%)] Loss: 2.301\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/50000 (38%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19840/50000 (40%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [20480/50000 (41%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [5120/50000 (10%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [5760/50000 (12%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/50000 (13%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [7040/50000 (14%)] Loss: 2.286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [7680/50000 (15%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [8320/50000 (17%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [21120/50000 (42%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [21760/50000 (43%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [22400/50000 (45%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [23040/50000 (46%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [23680/50000 (47%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [8960/50000 (18%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [9600/50000 (19%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10240/50000 (20%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10880/50000 (22%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [11520/50000 (23%)] Loss: 2.290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [24320/50000 (49%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [24960/50000 (50%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/50000 (51%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [26240/50000 (52%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [26880/50000 (54%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12160/50000 (24%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/50000 (26%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [13440/50000 (27%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [14080/50000 (28%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [14720/50000 (29%)] Loss: 2.286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [15360/50000 (31%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [27520/50000 (55%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [28160/50000 (56%)] Loss: 2.290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [28800/50000 (58%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [29440/50000 (59%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [30080/50000 (60%)] Loss: 2.298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [16000/50000 (32%)] Loss: 2.286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [16640/50000 (33%)] Loss: 2.276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [17280/50000 (35%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [17920/50000 (36%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [18560/50000 (37%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [30720/50000 (61%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [31360/50000 (63%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [32000/50000 (64%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [32640/50000 (65%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [33280/50000 (66%)] Loss: 2.302\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [33920/50000 (68%)] Loss: 2.297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/50000 (38%)] Loss: 2.294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19840/50000 (40%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [20480/50000 (41%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [21120/50000 (42%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [21760/50000 (43%)] Loss: 2.272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [22400/50000 (45%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [34560/50000 (69%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [35200/50000 (70%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [35840/50000 (72%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [36480/50000 (73%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [37120/50000 (74%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [37760/50000 (75%)] Loss: 2.296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [23040/50000 (46%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [23680/50000 (47%)] Loss: 2.291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [24320/50000 (49%)] Loss: 2.280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [24960/50000 (50%)] Loss: 2.282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/50000 (51%)] Loss: 2.284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [26240/50000 (52%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [38400/50000 (77%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [39040/50000 (78%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [39680/50000 (79%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [40320/50000 (81%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [40960/50000 (82%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [26880/50000 (54%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [27520/50000 (55%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [28160/50000 (56%)] Loss: 2.281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [28800/50000 (58%)] Loss: 2.284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [29440/50000 (59%)] Loss: 2.283\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [41600/50000 (83%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [42240/50000 (84%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [42880/50000 (86%)] Loss: 2.295\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [43520/50000 (87%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [44160/50000 (88%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [44800/50000 (90%)] Loss: 2.292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30080/50000 (60%)] Loss: 2.278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30720/50000 (61%)] Loss: 2.276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [31360/50000 (63%)] Loss: 2.269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/50000 (64%)] Loss: 2.275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32640/50000 (65%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [33280/50000 (66%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [45440/50000 (91%)] Loss: 2.300\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [46080/50000 (92%)] Loss: 2.299\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [46720/50000 (93%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [47360/50000 (95%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [48000/50000 (96%)] Loss: 2.295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [33920/50000 (68%)] Loss: 2.274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [34560/50000 (69%)] Loss: 2.274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [35200/50000 (70%)] Loss: 2.279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [35840/50000 (72%)] Loss: 2.283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [36480/50000 (73%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [37120/50000 (74%)] Loss: 2.286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [48640/50000 (97%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [49280/50000 (98%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [49920/50000 (100%)] Loss: 2.300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [37760/50000 (75%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/50000 (77%)] Loss: 2.288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [39040/50000 (78%)] Loss: 2.282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [39680/50000 (79%)] Loss: 2.272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40320/50000 (81%)] Loss: 2.272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40960/50000 (82%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [41600/50000 (83%)] Loss: 2.279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [42240/50000 (84%)] Loss: 2.284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [42880/50000 (86%)] Loss: 2.282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [43520/50000 (87%)] Loss: 2.264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44160/50000 (88%)] Loss: 2.266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/50000 (90%)] Loss: 2.273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [45440/50000 (91%)] Loss: 2.276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [46080/50000 (92%)] Loss: 2.278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [46720/50000 (93%)] Loss: 2.281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [47360/50000 (95%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [48000/50000 (96%)] Loss: 2.282\u001b[0m\n",
      "\u001b[35mTest set: Average loss: -0.0043, Accuracy: 11.65%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [640/50000 (1%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [1280/50000 (3%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [1920/50000 (4%)] Loss: 2.299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [48640/50000 (97%)] Loss: 2.250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [49280/50000 (98%)] Loss: 2.283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [49920/50000 (100%)] Loss: 2.284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [2560/50000 (5%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [3200/50000 (6%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [3840/50000 (8%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [4480/50000 (9%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [5120/50000 (10%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [5760/50000 (12%)] Loss: 2.294\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [6400/50000 (13%)] Loss: 2.298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [7040/50000 (14%)] Loss: 2.292\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [7680/50000 (15%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [8320/50000 (17%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [8960/50000 (18%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [9600/50000 (19%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [10240/50000 (20%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [10880/50000 (22%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [11520/50000 (23%)] Loss: 2.290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12160/50000 (24%)] Loss: 2.286\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/50000 (26%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [13440/50000 (27%)] Loss: 2.297\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [14080/50000 (28%)] Loss: 2.296\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [14720/50000 (29%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [15360/50000 (31%)] Loss: 2.290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [16000/50000 (32%)] Loss: 2.290\u001b[0m\n",
      "\u001b[34mTest set: Average loss: -0.0061, Accuracy: 16.24%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [640/50000 (1%)] Loss: 2.275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [1280/50000 (3%)] Loss: 2.275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [1920/50000 (4%)] Loss: 2.266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [2560/50000 (5%)] Loss: 2.283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [3200/50000 (6%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [3840/50000 (8%)] Loss: 2.276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [4480/50000 (9%)] Loss: 2.273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [5120/50000 (10%)] Loss: 2.278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [5760/50000 (12%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/50000 (13%)] Loss: 2.275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [7040/50000 (14%)] Loss: 2.268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [7680/50000 (15%)] Loss: 2.269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [8320/50000 (17%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [8960/50000 (18%)] Loss: 2.287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [9600/50000 (19%)] Loss: 2.289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [10240/50000 (20%)] Loss: 2.282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [10880/50000 (22%)] Loss: 2.271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [11520/50000 (23%)] Loss: 2.270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12160/50000 (24%)] Loss: 2.269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/50000 (26%)] Loss: 2.271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [13440/50000 (27%)] Loss: 2.272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [14080/50000 (28%)] Loss: 2.280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [14720/50000 (29%)] Loss: 2.262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [15360/50000 (31%)] Loss: 2.263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [16000/50000 (32%)] Loss: 2.259\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [34560/50000 (69%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [35200/50000 (70%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [35840/50000 (72%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [36480/50000 (73%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [37120/50000 (74%)] Loss: 2.284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [37760/50000 (75%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [38400/50000 (77%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [39040/50000 (78%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [39680/50000 (79%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [40320/50000 (81%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [40960/50000 (82%)] Loss: 2.293\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [41600/50000 (83%)] Loss: 2.278\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [42240/50000 (84%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [42880/50000 (86%)] Loss: 2.288\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [43520/50000 (87%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [44160/50000 (88%)] Loss: 2.283\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [44800/50000 (90%)] Loss: 2.285\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [45440/50000 (91%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [46080/50000 (92%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [46720/50000 (93%)] Loss: 2.291\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [47360/50000 (95%)] Loss: 2.289\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [48000/50000 (96%)] Loss: 2.287\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [48640/50000 (97%)] Loss: 2.277\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [49280/50000 (98%)] Loss: 2.284\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [49920/50000 (100%)] Loss: 2.293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [17920/50000 (36%)] Loss: 2.222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [18560/50000 (37%)] Loss: 2.216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/50000 (38%)] Loss: 2.231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19840/50000 (40%)] Loss: 2.194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [20480/50000 (41%)] Loss: 2.210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [21120/50000 (42%)] Loss: 2.224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [21760/50000 (43%)] Loss: 2.114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [22400/50000 (45%)] Loss: 2.220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [23040/50000 (46%)] Loss: 2.220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [23680/50000 (47%)] Loss: 2.213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [24320/50000 (49%)] Loss: 2.169\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [24960/50000 (50%)] Loss: 2.197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/50000 (51%)] Loss: 2.222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [26240/50000 (52%)] Loss: 2.210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [26880/50000 (54%)] Loss: 2.137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [27520/50000 (55%)] Loss: 2.163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [28160/50000 (56%)] Loss: 2.170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [28800/50000 (58%)] Loss: 2.201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [29440/50000 (59%)] Loss: 2.163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [30080/50000 (60%)] Loss: 2.163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [30720/50000 (61%)] Loss: 2.107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [31360/50000 (63%)] Loss: 2.066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/50000 (64%)] Loss: 2.135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32640/50000 (65%)] Loss: 2.217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [33280/50000 (66%)] Loss: 2.158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [33920/50000 (68%)] Loss: 2.146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [34560/50000 (69%)] Loss: 2.128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [35200/50000 (70%)] Loss: 2.121\u001b[0m\n",
      "\u001b[35mTest set: Average loss: -0.0430, Accuracy: 19.03%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [640/50000 (1%)] Loss: 2.253\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [1280/50000 (3%)] Loss: 2.259\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [1920/50000 (4%)] Loss: 2.262\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [2560/50000 (5%)] Loss: 2.261\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [3200/50000 (6%)] Loss: 2.262\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [3840/50000 (8%)] Loss: 2.245\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [4480/50000 (9%)] Loss: 2.251\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [5120/50000 (10%)] Loss: 2.263\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [5760/50000 (12%)] Loss: 2.253\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [6400/50000 (13%)] Loss: 2.235\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [7040/50000 (14%)] Loss: 2.244\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [7680/50000 (15%)] Loss: 2.251\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [8320/50000 (17%)] Loss: 2.258\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [8960/50000 (18%)] Loss: 2.273\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [9600/50000 (19%)] Loss: 2.277\u001b[0m\n",
      "\u001b[34mTest set: Average loss: -0.1965, Accuracy: 21.68%\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [10240/50000 (20%)] Loss: 2.267\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [10880/50000 (22%)] Loss: 2.254\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [11520/50000 (23%)] Loss: 2.223\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [12160/50000 (24%)] Loss: 2.217\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [12800/50000 (26%)] Loss: 2.259\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [13440/50000 (27%)] Loss: 2.268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [640/50000 (1%)] Loss: 2.098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [1280/50000 (3%)] Loss: 2.118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [1920/50000 (4%)] Loss: 2.093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [2560/50000 (5%)] Loss: 2.179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [3200/50000 (6%)] Loss: 2.118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [3840/50000 (8%)] Loss: 2.111\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [14080/50000 (28%)] Loss: 2.265\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [14720/50000 (29%)] Loss: 2.215\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [15360/50000 (31%)] Loss: 2.248\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [16000/50000 (32%)] Loss: 2.204\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [16640/50000 (33%)] Loss: 2.217\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [17280/50000 (35%)] Loss: 2.285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [4480/50000 (9%)] Loss: 2.155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [5120/50000 (10%)] Loss: 2.133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [5760/50000 (12%)] Loss: 2.195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/50000 (13%)] Loss: 2.134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [7040/50000 (14%)] Loss: 2.128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [7680/50000 (15%)] Loss: 2.158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [8320/50000 (17%)] Loss: 2.189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [8960/50000 (18%)] Loss: 2.154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [9600/50000 (19%)] Loss: 2.244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [10240/50000 (20%)] Loss: 2.114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [10880/50000 (22%)] Loss: 2.092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [11520/50000 (23%)] Loss: 2.089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12160/50000 (24%)] Loss: 2.127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/50000 (26%)] Loss: 2.094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [13440/50000 (27%)] Loss: 2.149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [14080/50000 (28%)] Loss: 2.164\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [14720/50000 (29%)] Loss: 2.092\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [15360/50000 (31%)] Loss: 2.173\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [16000/50000 (32%)] Loss: 2.090\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [16640/50000 (33%)] Loss: 2.146\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [17280/50000 (35%)] Loss: 2.244\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [17920/50000 (36%)] Loss: 2.166\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [18560/50000 (37%)] Loss: 2.228\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [19200/50000 (38%)] Loss: 2.165\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [19840/50000 (40%)] Loss: 2.114\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [20480/50000 (41%)] Loss: 2.162\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [21120/50000 (42%)] Loss: 2.177\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [21760/50000 (43%)] Loss: 2.039\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [22400/50000 (45%)] Loss: 2.156\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [23040/50000 (46%)] Loss: 2.190\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [23680/50000 (47%)] Loss: 2.124\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [24320/50000 (49%)] Loss: 2.129\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [24960/50000 (50%)] Loss: 2.168\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [25600/50000 (51%)] Loss: 2.166\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [26240/50000 (52%)] Loss: 2.180\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [26880/50000 (54%)] Loss: 2.120\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [27520/50000 (55%)] Loss: 2.105\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [28160/50000 (56%)] Loss: 2.121\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [28800/50000 (58%)] Loss: 2.150\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [29440/50000 (59%)] Loss: 2.127\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [30080/50000 (60%)] Loss: 2.071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [30720/50000 (61%)] Loss: 2.066\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [31360/50000 (63%)] Loss: 1.994\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [32000/50000 (64%)] Loss: 2.113\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [32640/50000 (65%)] Loss: 2.211\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [33280/50000 (66%)] Loss: 2.152\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [33920/50000 (68%)] Loss: 2.092\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [34560/50000 (69%)] Loss: 2.096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [35200/50000 (70%)] Loss: 2.075\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [35840/50000 (72%)] Loss: 2.068\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [36480/50000 (73%)] Loss: 2.081\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [37120/50000 (74%)] Loss: 2.083\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [37760/50000 (75%)] Loss: 2.066\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [38400/50000 (77%)] Loss: 2.117\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [39040/50000 (78%)] Loss: 2.170\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [39680/50000 (79%)] Loss: 2.065\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [40320/50000 (81%)] Loss: 2.037\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [40960/50000 (82%)] Loss: 2.154\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [41600/50000 (83%)] Loss: 2.035\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [42240/50000 (84%)] Loss: 2.112\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [42880/50000 (86%)] Loss: 2.138\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [43520/50000 (87%)] Loss: 2.105\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [44160/50000 (88%)] Loss: 2.024\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [44800/50000 (90%)] Loss: 2.047\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [45440/50000 (91%)] Loss: 2.065\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [46080/50000 (92%)] Loss: 2.073\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [46720/50000 (93%)] Loss: 2.071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [47360/50000 (95%)] Loss: 2.134\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [48000/50000 (96%)] Loss: 2.069\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [48640/50000 (97%)] Loss: 2.004\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [49280/50000 (98%)] Loss: 2.107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32640/50000 (65%)] Loss: 2.106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [33280/50000 (66%)] Loss: 1.990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [33920/50000 (68%)] Loss: 2.026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [34560/50000 (69%)] Loss: 2.037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [35200/50000 (70%)] Loss: 1.973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [35840/50000 (72%)] Loss: 1.955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [36480/50000 (73%)] Loss: 1.961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [37120/50000 (74%)] Loss: 2.078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [37760/50000 (75%)] Loss: 1.960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/50000 (77%)] Loss: 2.053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [39040/50000 (78%)] Loss: 2.137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [39680/50000 (79%)] Loss: 1.984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [40320/50000 (81%)] Loss: 1.968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [40960/50000 (82%)] Loss: 2.027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [41600/50000 (83%)] Loss: 1.970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [42240/50000 (84%)] Loss: 2.099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [42880/50000 (86%)] Loss: 2.125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [43520/50000 (87%)] Loss: 2.000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44160/50000 (88%)] Loss: 1.897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/50000 (90%)] Loss: 1.899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [45440/50000 (91%)] Loss: 1.962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [46080/50000 (92%)] Loss: 1.991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [46720/50000 (93%)] Loss: 2.000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [47360/50000 (95%)] Loss: 2.043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [48000/50000 (96%)] Loss: 2.062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [48640/50000 (97%)] Loss: 1.861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [49280/50000 (98%)] Loss: 2.061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [49920/50000 (100%)] Loss: 2.072\u001b[0m\n",
      "\u001b[34mTest set: Average loss: -0.4429, Accuracy: 26.02%\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [640/50000 (1%)] Loss: 1.935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [1280/50000 (3%)] Loss: 1.947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [1920/50000 (4%)] Loss: 1.998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [2560/50000 (5%)] Loss: 2.033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [3200/50000 (6%)] Loss: 1.991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [3840/50000 (8%)] Loss: 2.006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [4480/50000 (9%)] Loss: 2.025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [5120/50000 (10%)] Loss: 1.972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [5760/50000 (12%)] Loss: 2.125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/50000 (13%)] Loss: 2.004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [7040/50000 (14%)] Loss: 2.035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [7680/50000 (15%)] Loss: 2.073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [8320/50000 (17%)] Loss: 2.117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [8960/50000 (18%)] Loss: 2.045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [9600/50000 (19%)] Loss: 2.177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [10240/50000 (20%)] Loss: 1.998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [10880/50000 (22%)] Loss: 1.981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [11520/50000 (23%)] Loss: 1.968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12160/50000 (24%)] Loss: 2.041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/50000 (26%)] Loss: 1.951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [13440/50000 (27%)] Loss: 2.033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [14080/50000 (28%)] Loss: 2.024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [14720/50000 (29%)] Loss: 1.889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [15360/50000 (31%)] Loss: 1.989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [16000/50000 (32%)] Loss: 2.047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [16640/50000 (33%)] Loss: 2.096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [17280/50000 (35%)] Loss: 2.110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [17920/50000 (36%)] Loss: 2.005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [18560/50000 (37%)] Loss: 2.084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/50000 (38%)] Loss: 2.139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19840/50000 (40%)] Loss: 1.954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [20480/50000 (41%)] Loss: 2.132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [21120/50000 (42%)] Loss: 2.085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [21760/50000 (43%)] Loss: 1.857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [22400/50000 (45%)] Loss: 2.151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [23040/50000 (46%)] Loss: 2.030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [23680/50000 (47%)] Loss: 2.015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [24320/50000 (49%)] Loss: 1.978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [24960/50000 (50%)] Loss: 2.040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/50000 (51%)] Loss: 2.095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [26240/50000 (52%)] Loss: 2.089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [26880/50000 (54%)] Loss: 1.928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [27520/50000 (55%)] Loss: 1.993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [28160/50000 (56%)] Loss: 1.985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [28800/50000 (58%)] Loss: 2.062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [29440/50000 (59%)] Loss: 1.973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [30080/50000 (60%)] Loss: 2.004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [30720/50000 (61%)] Loss: 1.893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [31360/50000 (63%)] Loss: 1.827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/50000 (64%)] Loss: 1.966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32640/50000 (65%)] Loss: 2.092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [33280/50000 (66%)] Loss: 1.946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [33920/50000 (68%)] Loss: 1.995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [34560/50000 (69%)] Loss: 2.017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [35200/50000 (70%)] Loss: 1.937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [35840/50000 (72%)] Loss: 1.903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [36480/50000 (73%)] Loss: 1.914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [37120/50000 (74%)] Loss: 2.043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [37760/50000 (75%)] Loss: 1.921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/50000 (77%)] Loss: 2.007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [39040/50000 (78%)] Loss: 2.129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [39680/50000 (79%)] Loss: 1.913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [40320/50000 (81%)] Loss: 1.942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [40960/50000 (82%)] Loss: 1.993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [41600/50000 (83%)] Loss: 1.934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [42240/50000 (84%)] Loss: 2.059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [42880/50000 (86%)] Loss: 2.078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [43520/50000 (87%)] Loss: 1.979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44160/50000 (88%)] Loss: 1.854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/50000 (90%)] Loss: 1.865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [45440/50000 (91%)] Loss: 1.952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [46080/50000 (92%)] Loss: 1.965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [46720/50000 (93%)] Loss: 1.946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [47360/50000 (95%)] Loss: 2.013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [48000/50000 (96%)] Loss: 2.013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [48640/50000 (97%)] Loss: 1.840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [49280/50000 (98%)] Loss: 2.021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [49920/50000 (100%)] Loss: 2.029\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-27 11:40:20,003 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-09-27 11:40:48 Uploading - Uploading generated training model\n",
      "2021-09-27 11:40:48 Completed - Training job completed\n",
      "ProfilerReport-1632742342: IssuesFound\n",
      "Training seconds: 610\n",
      "Billable seconds: 610\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p3.8xlarge'\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train_horovod.py\",    \n",
    "    source_dir='source',    \n",
    "    base_job_name = job_name,\n",
    "    role=role,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    train_instance_count=2,\n",
    "    train_instance_type=instance_type,\n",
    "    hyperparameters={\"epochs\": 10, \n",
    "                     'lr': 0.001,\n",
    "                     'batch-size': 64,\n",
    "                     \"backend\": \"gloo\",                     \n",
    "                    },    \n",
    ")\n",
    "cifar10_estimator.fit({\"training\" : inputs, wait=False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 정리 작업\n",
    "\n",
    "## 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horovod_artifact_path:  s3://sagemaker-us-east-1-057716757052/cifar10-horovod-2021-09-27-11-32-22-045/output/model.tar.gz\n",
      "Stored 'horovod_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "horovod_artifact_path = cifar10_estimator.model_data\n",
    "print(\"horovod_artifact_path: \", horovod_artifact_path)\n",
    "\n",
    "\n",
    "%store horovod_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 11:40:28     461492 cifar10-horovod-2021-09-27-11-32-22-045/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {horovod_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
