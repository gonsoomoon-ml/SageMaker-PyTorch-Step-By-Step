{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3368c74f",
   "metadata": {},
   "source": [
    "# [Module 1.5] 체크 포인트를 활용한 훈련\n",
    "\n",
    "본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 아래는 세이지메이커의 어떤 피쳐도 사용하지 않고, PyTorch 만을 사용해서 훈련 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192377c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.45.0\n",
      "Checkpointing Path: s3://sagemaker-ap-northeast-2-057716757052/checkpoint-23753227\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print('SageMaker version: ' + sagemaker.__version__)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-cnn-cifar10'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "\n",
    "print('Checkpointing Path: {}'.format(checkpoint_s3_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a6f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f905c4",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb1aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 inputs:  s3://sagemaker-ap-northeast-2-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"../data\", bucket=bucket, key_prefix=\"data/cifar10\")\n",
    "print(\"s3 inputs: \", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "459dd532",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 6nvhc7dnes-algo-1-7y9kb ... \n",
      "Creating 6nvhc7dnes-algo-1-7y9kb ... done\n",
      "Attaching to 6nvhc7dnes-algo-1-7y9kb\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:56,701 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:56,744 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:56,746 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:56,779 botocore.credentials INFO     Found credentials in environment variables.\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:56,888 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Requirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Collecting torchsummary==1.5.1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Installing collected packages: torchsummary\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Successfully installed torchsummary-1.5.1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:30:58,997 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Training Env:\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     },\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"current_host\": \"algo-1-7y9kb\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"algo-1-7y9kb\"\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     ],\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"epochs\": 1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     },\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"training\": {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         }\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     },\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"job_name\": \"cifar10-pytorch-spot-1-2021-07-28-13-30-49-900\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"master_hostname\": \"algo-1-7y9kb\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-13-30-49-900/source/sourcedir.tar.gz\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"module_name\": \"cifar10-spot\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"num_gpus\": 4,\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"current_host\": \"algo-1-7y9kb\",\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m             \"algo-1-7y9kb\"\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m         ]\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     },\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m     \"user_entry_point\": \"cifar10-spot.py\"\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m }\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Environment variables:\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_HOSTS=[\"algo-1-7y9kb\"]\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_HPS={\"epochs\":1}\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_USER_ENTRY_POINT=cifar10-spot.py\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-7y9kb\",\"hosts\":[\"algo-1-7y9kb\"]}\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_CURRENT_HOST=algo-1-7y9kb\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_MODULE_NAME=cifar10-spot\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_NUM_GPUS=4\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-13-30-49-900/source/sourcedir.tar.gz\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-7y9kb\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-7y9kb\"],\"hyperparameters\":{\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-pytorch-spot-1-2021-07-28-13-30-49-900\",\"log_level\":20,\"master_hostname\":\"algo-1-7y9kb\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-13-30-49-900/source/sourcedir.tar.gz\",\"module_name\":\"cifar10-spot\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-7y9kb\",\"hosts\":[\"algo-1-7y9kb\"]},\"user_entry_point\":\"cifar10-spot.py\"}\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\"]\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m /opt/conda/bin/python3.6 cifar10-spot.py --epochs 1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Distributed training - False\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Creating Checkpointing directory /opt/ml/checkpoints\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Device Type: cuda\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Loading Cifar10 dataset\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Model loaded from get_model_network()\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [2021-07-28 13:31:02.493 algo-1-7y9kb:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [2021-07-28 13:31:02.727 algo-1-7y9kb:31 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m NCCL version 2.4.8+cuda10.1\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1,  2000] loss: 2.249\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1,  4000] loss: 2.127\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1,  6000] loss: 2.033\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1,  8000] loss: 1.973\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1, 10000] loss: 1.960\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m [1, 12000] loss: 1.900\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m epoch: 1 - loss: 2.267782688140869\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Saving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Finished Training\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m Saving the model.\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m \n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb |\u001b[0m 2021-07-28 13:33:28,504 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36m6nvhc7dnes-algo-1-7y9kb exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {'epochs': 1}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "spot_estimator = PyTorch(\n",
    "                            entry_point='cifar10-spot.py',\n",
    "                            source_dir='source',                                                            \n",
    "                            role=role,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "                            instance_type='local_gpu',\n",
    "                            base_job_name='cifar10-pytorch-spot-1',\n",
    "                            hyperparameters=hyperparameters,\n",
    " \n",
    " \n",
    ")\n",
    "\n",
    "spot_estimator.fit(inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45f21b",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1c8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_type:  local_gpu\n",
      "role:  arn:aws:iam::057716757052:role/secure-vpc-client\n"
     ]
    }
   ],
   "source": [
    "print(\"instance_type: \", instance_type)\n",
    "print(\"role: \", role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb425ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = True\n",
    "max_run=600\n",
    "max_wait = 1200 if use_spot_instances else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f2e65",
   "metadata": {},
   "source": [
    "## Simulating Spot interruption after 5 epochs\n",
    "\n",
    "Our training job should run on 10 epochs.\n",
    "\n",
    "However, we will simulate a situation that after 5 epochs a spot interruption occurred.\n",
    "\n",
    "The goal is that the checkpointing data will be copied to S3, so when there is a spot capacity available again, the training job can resume from the 6th epoch.\n",
    "\n",
    "Note the `checkpoint_s3_uri` variable which stores the S3 URI in which to persist checkpoints that the algorithm persists (if any) during training.\n",
    "\n",
    "The `debugger_hook_config` parameter must be set to `False` to enable checkpoints to be copied to S3 successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 5}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "spot_estimator = PyTorch(\n",
    "                            entry_point='cifar10.py',\n",
    "                            source_dir='source',                                                            \n",
    "                            role=role,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            base_job_name='cifar10-pytorch-spot-1',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                            debugger_hook_config=False,\n",
    "                            use_spot_instances=use_spot_instances,\n",
    "                            max_run=max_run,\n",
    "                            max_wait=max_wait)\n",
    "\n",
    "spot_estimator.fit(inputs, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44f8e20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-28 12:52:09 Starting - Launching requested ML instances...ProfilerReport-1627476727: InProgress\n",
      "......\n",
      "2021-07-28 12:53:32 Starting - Preparing the instances for training......\n",
      "2021-07-28 12:54:47 Downloading - Downloading input data...\n",
      "2021-07-28 12:55:13 Training - Downloading the training image.........\n",
      "2021-07-28 12:56:42 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-07-28 12:56:43,253 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-07-28 12:56:43,277 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-07-28 12:56:43,284 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-28 12:56:43,641 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[34mCollecting torchsummary==1.5.1\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torchsummary\u001b[0m\n",
      "\u001b[34mSuccessfully installed torchsummary-1.5.1\u001b[0m\n",
      "\u001b[34m2021-07-28 12:56:46,029 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-pytorch-spot-1-2021-07-28-12-52-07-632\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-12-52-07-632/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-12-52-07-632/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-pytorch-spot-1-2021-07-28-12-52-07-632\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-12-52-07-632/source/sourcedir.tar.gz\",\"module_name\":\"cifar10\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 cifar10.py --epochs 5\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCheckpointing directory /opt/ml/checkpoints exists\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoading Cifar10 dataset\u001b[0m\n",
      "\u001b[34mModel loaded\u001b[0m\n",
      "\u001b[34m[2021-07-28 12:56:52.086 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-07-28 12:56:52.389 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,  2000] loss: 2.252\u001b[0m\n",
      "\u001b[34m[1,  4000] loss: 2.118\u001b[0m\n",
      "\u001b[34m[1,  6000] loss: 2.015\u001b[0m\n",
      "\u001b[34m[1,  8000] loss: 1.961\u001b[0m\n",
      "\u001b[34m[1, 10000] loss: 1.921\u001b[0m\n",
      "\u001b[34m[1, 12000] loss: 1.917\u001b[0m\n",
      "\u001b[34mepoch: 1 - loss: 1.809316873550415\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[2,  2000] loss: 1.877\u001b[0m\n",
      "\u001b[34m[2,  4000] loss: 1.859\u001b[0m\n",
      "\u001b[34m[2,  6000] loss: 1.843\u001b[0m\n",
      "\u001b[34m[2,  8000] loss: 1.837\u001b[0m\n",
      "\u001b[34m[2, 10000] loss: 1.824\u001b[0m\n",
      "\u001b[34m[2, 12000] loss: 1.802\u001b[0m\n",
      "\u001b[34mepoch: 2 - loss: 1.9053090810775757\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[3,  2000] loss: 1.779\u001b[0m\n",
      "\u001b[34m[3,  4000] loss: 1.769\u001b[0m\n",
      "\u001b[34m[3,  6000] loss: 1.784\u001b[0m\n",
      "\u001b[34m[3,  8000] loss: 1.767\u001b[0m\n",
      "\u001b[34m[3, 10000] loss: 1.749\u001b[0m\n",
      "\u001b[34m[3, 12000] loss: 1.765\u001b[0m\n",
      "\u001b[34mepoch: 3 - loss: 1.1225974559783936\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[4,  2000] loss: 1.699\u001b[0m\n",
      "\u001b[34m[4,  4000] loss: 1.734\u001b[0m\n",
      "\u001b[34m[4,  6000] loss: 1.724\u001b[0m\n",
      "\u001b[34m[4,  8000] loss: 1.699\u001b[0m\n",
      "\u001b[34m[4, 10000] loss: 1.724\u001b[0m\n",
      "\u001b[34m[4, 12000] loss: 1.721\u001b[0m\n",
      "\u001b[34mepoch: 4 - loss: 1.3991646766662598\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[5,  2000] loss: 1.679\u001b[0m\n",
      "\u001b[34m[5,  4000] loss: 1.678\u001b[0m\n",
      "\u001b[34m[5,  6000] loss: 1.663\u001b[0m\n",
      "\u001b[34m[5,  8000] loss: 1.680\u001b[0m\n",
      "\u001b[34m[5, 10000] loss: 1.668\u001b[0m\n",
      "\u001b[34m[5, 12000] loss: 1.673\u001b[0m\n",
      "\u001b[34mepoch: 5 - loss: 2.22383451461792\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34mFinished Training\u001b[0m\n",
      "\u001b[34mSaving the model.\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-28 13:01:55,748 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-07-28 13:02:14 Uploading - Uploading generated training model\n",
      "2021-07-28 13:02:14 Completed - Training job completed\n",
      "Training seconds: 439\n",
      "Billable seconds: 132\n",
      "Managed Spot Training savings: 69.9%\n"
     ]
    }
   ],
   "source": [
    "spot_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a98797",
   "metadata": {},
   "source": [
    "### View the job training Checkpoint configuration\n",
    "We can now view the Checkpoint configuration from the training job directly in the SageMaker console.\n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the Checkpoint configuration section. \n",
    "\n",
    "Choose the S3 output path link and you'll be directed to the S3 bucket were checkpointing data is saved.\n",
    "\n",
    "You can see there is one file there:\n",
    "\n",
    "```python\n",
    "checkpoint.pth\n",
    "```\n",
    "\n",
    "This is the checkpoint file that contains the epoch, model state dict, optimizer state dict, and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94dc1ca",
   "metadata": {},
   "source": [
    "### Continue training after Spot capacity is resumed\n",
    "\n",
    "Now we simulate a situation where Spot capacity is resumed.\n",
    "\n",
    "We will start a training job again, this time with 10 epochs.\n",
    "\n",
    "What we expect is that the tarining job will start from the 6th epoch.\n",
    "\n",
    "This is done when training job starts. It checks the checkpoint s3 location for checkpoints data. If there are, they are copied to `/opt/ml/checkpoints` on the training conatiner.\n",
    "\n",
    "In the code you can see the function to load the checkpoints data:\n",
    "\n",
    "```python\n",
    "def _load_checkpoint(model, optimizer, args):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Checkpoint file found!\")\n",
    "    print(\"Loading Checkpoint From: {}\".format(args.checkpoint_path + '/checkpoint.pth'))\n",
    "    checkpoint = torch.load(args.checkpoint_path + '/checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch_number = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"Checkpoint File Loaded - epoch_number: {} - loss: {}\".format(epoch_number, loss))\n",
    "    print('Resuming training from epoch: {}'.format(epoch_number+1))\n",
    "    print(\"--------------------------------------------\")\n",
    "    return model, optimizer, epoch_number\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2004ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 10}\n",
    "\n",
    "\n",
    "spot_estimator = PyTorch(entry_point='cifar10.py',\n",
    "                            source_dir='source',                                                                                     \n",
    "                            role=role,\n",
    "                            framework_version='1.7.1',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            base_job_name='cifar10-pytorch-spot-2',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                            debugger_hook_config=False,\n",
    "                            use_spot_instances=use_spot_instances,\n",
    "                            max_run=max_run,\n",
    "                            max_wait=max_wait)\n",
    "\n",
    "spot_estimator.fit(inputs, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08a31fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-28 12:32:35 Starting - Starting the training job...\n",
      "2021-07-28 12:32:59 Starting - Launching requested ML instancesProfilerReport-1627475555: InProgress\n",
      "......\n",
      "2021-07-28 12:34:03 Starting - Preparing the instances for training......\n",
      "2021-07-28 12:35:06 Downloading - Downloading input data...\n",
      "2021-07-28 12:35:24 Training - Downloading the training image..................\n",
      "2021-07-28 12:38:35 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-07-28 12:38:28,784 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-07-28 12:38:28,808 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-07-28 12:38:31,835 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-28 12:38:32,177 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting torchvision==0.7.0\n",
      "  Downloading torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting torchsummary==1.5.1\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torch, torchvision, torchsummary\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\u001b[0m\n",
      "\u001b[34m    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.8.2\n",
      "    Uninstalling torchvision-0.8.2:\n",
      "      Successfully uninstalled torchvision-0.8.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed torch-1.6.0 torchsummary-1.5.1 torchvision-0.7.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-28 12:39:21,975 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-pytorch-spot-2-2021-07-28-12-32-34-896\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-2-2021-07-28-12-32-34-896/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-2-2021-07-28-12-32-34-896/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-pytorch-spot-2-2021-07-28-12-32-34-896\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-2-2021-07-28-12-32-34-896/source/sourcedir.tar.gz\",\"module_name\":\"cifar10\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 cifar10.py --epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCheckpointing directory /opt/ml/checkpoints exists\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoading Cifar10 dataset\u001b[0m\n",
      "\u001b[34mModel loaded\u001b[0m\n",
      "\u001b[34m--------------------------------------------\u001b[0m\n",
      "\u001b[34mCheckpoint file found!\u001b[0m\n",
      "\u001b[34mLoading Checkpoint From: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34mCheckpoint File Loaded - epoch_number: 5 - loss: 1.0219937562942505\u001b[0m\n",
      "\u001b[34mResuming training from epoch: 6\u001b[0m\n",
      "\u001b[34m--------------------------------------------\u001b[0m\n",
      "\u001b[34m[6,  2000] loss: 0.956\u001b[0m\n",
      "\u001b[34m[6,  4000] loss: 0.976\u001b[0m\n",
      "\u001b[34m[6,  6000] loss: 0.974\u001b[0m\n",
      "\u001b[34m[6,  8000] loss: 0.998\u001b[0m\n",
      "\u001b[34m[6, 10000] loss: 0.989\u001b[0m\n",
      "\u001b[34m[6, 12000] loss: 1.007\u001b[0m\n",
      "\u001b[34mepoch: 6 - loss: 1.3371285200119019\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[7,  2000] loss: 0.882\u001b[0m\n",
      "\u001b[34m[7,  4000] loss: 0.929\u001b[0m\n",
      "\u001b[34m[7,  6000] loss: 0.946\u001b[0m\n",
      "\u001b[34m[7,  8000] loss: 0.961\u001b[0m\n",
      "\u001b[34m[7, 10000] loss: 0.940\u001b[0m\n",
      "\u001b[34m[7, 12000] loss: 0.952\u001b[0m\n",
      "\u001b[34mepoch: 7 - loss: 1.406465768814087\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[8,  2000] loss: 0.860\u001b[0m\n",
      "\u001b[34m[8,  4000] loss: 0.873\u001b[0m\n",
      "\u001b[34m[8,  6000] loss: 0.880\u001b[0m\n",
      "\u001b[34m[8,  8000] loss: 0.898\u001b[0m\n",
      "\u001b[34m[8, 10000] loss: 0.929\u001b[0m\n",
      "\u001b[34m[8, 12000] loss: 0.899\u001b[0m\n",
      "\u001b[34mepoch: 8 - loss: 1.788290023803711\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[9,  2000] loss: 0.814\u001b[0m\n",
      "\u001b[34m[9,  4000] loss: 0.839\u001b[0m\n",
      "\u001b[34m[9,  6000] loss: 0.873\u001b[0m\n",
      "\u001b[34m[9,  8000] loss: 0.861\u001b[0m\n",
      "\u001b[34m[9, 10000] loss: 0.864\u001b[0m\n",
      "\u001b[34m[9, 12000] loss: 0.896\u001b[0m\n",
      "\u001b[34mepoch: 9 - loss: 0.5536425709724426\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34m[10,  2000] loss: 0.777\u001b[0m\n",
      "\u001b[34m[10,  4000] loss: 0.810\u001b[0m\n",
      "\u001b[34m[10,  6000] loss: 0.826\u001b[0m\n",
      "\u001b[34m[10,  8000] loss: 0.852\u001b[0m\n",
      "\u001b[34m[10, 10000] loss: 0.855\u001b[0m\n",
      "\u001b[34m[10, 12000] loss: 0.868\u001b[0m\n",
      "\u001b[34mepoch: 10 - loss: 0.2988367974758148\u001b[0m\n",
      "\u001b[34mSaving the Checkpoint: /opt/ml/checkpoints/checkpoint.pth\u001b[0m\n",
      "\u001b[34mFinished Training\u001b[0m\n",
      "\u001b[34mSaving the model.\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-28 12:43:34,383 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-07-28 12:44:02 Uploading - Uploading generated training model\n",
      "2021-07-28 12:44:02 Completed - Training job completed\n",
      "ProfilerReport-1627475555: NoIssuesFound\n",
      "Training seconds: 523\n",
      "Billable seconds: 157\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "spot_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c89e90",
   "metadata": {},
   "source": [
    "### Analyze training job logs\n",
    "\n",
    "Analyzing the training job logs, we can see that now, the training job starts from the 6th epoch.\n",
    "\n",
    "We can see the output of `_load_checkpoint` function:\n",
    "\n",
    "```\n",
    "--------------------------------------------\n",
    "Checkpoint file found!\n",
    "Loading Checkpoint From: /opt/ml/checkpoints/checkpoint.pth\n",
    "Checkpoint File Loaded - epoch_number: 5 - loss: 0.8455273509025574\n",
    "Resuming training from epoch: 6\n",
    "--------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85eac5",
   "metadata": {},
   "source": [
    "### View the job training Checkpoint configuration after job completed 10 epochs\n",
    "\n",
    "We can now view the Checkpoint configuration from the training job directly in the SageMaker console.  \n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the Checkpoint configuration section. \n",
    "\n",
    "Choose the S3 output path link and you'll be directed to the S3 bucket were checkpointing data is saved.\n",
    "\n",
    "You can see there is still that one file there:\n",
    "\n",
    "```python\n",
    "checkpoint.pth\n",
    "```\n",
    "\n",
    "You'll be able to see that the date of the checkpoint file was updated to the time of the 2nd Spot training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f654b6d",
   "metadata": {},
   "source": [
    "## 모델 아티펙트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2029af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spot_artifact_path:  s3://sagemaker-ap-northeast-2-057716757052/cifar10-pytorch-spot-1-2021-07-28-13-30-49-900/model.tar.gz\n",
      "Stored 'spot_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "spot_artifact_path = spot_estimator.model_data\n",
    "print(\"spot_artifact_path: \", spot_artifact_path)\n",
    "\n",
    "%store spot_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e191b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1c617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
