{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1.2] 로컬 모드 및 스크립트 모드로 훈련 (SageMaker 사용)\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 기본 환경 세팅\n",
    "- 데이터 세트를 S3에 업로드\n",
    "- 스크립트 모드의 코드 작성 방법\n",
    "- 훈련 코드 확인\n",
    "- 로컬 모드로 훈련 실행\n",
    "- SageMaker Host Mode 로 훈련\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r local_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 inputs:  s3://sagemaker-us-east-1-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"../data\", bucket=bucket, key_prefix=\"data/cifar10\")\n",
    "print(\"s3 inputs: \", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 14:12:16        158 data/cifar10/cifar-10-batches-py/batches.meta\n",
      "2021-09-27 14:12:12   31035704 data/cifar10/cifar-10-batches-py/data_batch_1\n",
      "2021-09-27 14:12:10   31035320 data/cifar10/cifar-10-batches-py/data_batch_2\n",
      "2021-09-27 14:12:12   31035999 data/cifar10/cifar-10-batches-py/data_batch_3\n",
      "2021-09-27 14:12:11   31035696 data/cifar10/cifar-10-batches-py/data_batch_4\n",
      "2021-09-27 14:12:11   31035623 data/cifar10/cifar-10-batches-py/data_batch_5\n",
      "2021-09-27 14:12:11         88 data/cifar10/cifar-10-batches-py/readme.html\n",
      "2021-09-27 14:12:10   31035526 data/cifar10/cifar-10-batches-py/test_batch\n",
      "2021-09-27 14:12:09  170498071 data/cifar10/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {inputs} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스크립트 모드의 코드 작성 방법\n",
    "- ![script_mode_example.png](img/script_mode_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 코드 확인\n",
    "- 아래의 코드는 전형적인 스크립트 모드의 코드 작성 방법을 따르고 있습니다.\n",
    "- 훈련 함수는 `from train_lib import train` 로서 이전 노트북의 **[세이지 메이커 없이]** 작성한 스크래치 버전에서 사용한 훈련 함수와 동일 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrain_lib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "    \u001b[37m#### 사용자 정의 커맨드 인자\u001b[39;49;00m\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "    \n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m2\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mW\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of data loading workers (default: 2)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m1\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of total epochs to run (default: 2)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mBS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch size (default: 4)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.001\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minitial learning rate (default: 0.001)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmomentum (default: 0.9)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--dist_backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgloo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mdistributed backend (default: gloo)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    \n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "    \u001b[37m#### 세이지 메이커 프레임워크의 도커 컨테이너 환경 변수 인자\u001b[39;49;00m\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "       \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "\n",
      "    \n",
      "    \u001b[37m# parse arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args() \n",
      "    \n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "    \u001b[37m#### 훈련 함수 콜\u001b[39;49;00m\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "    \n",
      "    train(args)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_code = 'source/train.py'\n",
    "!pygmentize {train_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬에 있는 데이타 세트의 위치를 지정 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_inputs = \"file://../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rz56mlhkp9-algo-1-yx9pc ... \n",
      "Creating rz56mlhkp9-algo-1-yx9pc ... done\n",
      "Attaching to rz56mlhkp9-algo-1-yx9pc\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:12:49,852 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:12:49,932 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:12:49,935 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:12:50,134 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: torch==1.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.8.1)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: torchvision==0.9.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.9.1)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Collecting torchsummary==1.5.1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (0.8)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (3.10.0.0)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.9.1->-r requirements.txt (line 4)) (8.3.1)\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Installing collected packages: torchsummary\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Successfully installed torchsummary-1.5.1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:12:52,156 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Training Env:\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     },\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"current_host\": \"algo-1-yx9pc\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"algo-1-yx9pc\"\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     ],\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"lr\": 0.1,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"batch_size\": 16\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     },\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"training\": {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         }\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     },\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"job_name\": \"pytorch-training-2021-09-27-14-12-46-350\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"master_hostname\": \"algo-1-yx9pc\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-12-46-350/source/sourcedir.tar.gz\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"current_host\": \"algo-1-yx9pc\",\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m             \"algo-1-yx9pc\"\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m         ]\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     },\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m }\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Environment variables:\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_HOSTS=[\"algo-1-yx9pc\"]\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_HPS={\"batch_size\":16,\"epochs\":1,\"lr\":0.1}\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-yx9pc\",\"hosts\":[\"algo-1-yx9pc\"]}\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_CURRENT_HOST=algo-1-yx9pc\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-12-46-350/source/sourcedir.tar.gz\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-yx9pc\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-yx9pc\"],\"hyperparameters\":{\"batch_size\":16,\"epochs\":1,\"lr\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-09-27-14-12-46-350\",\"log_level\":20,\"master_hostname\":\"algo-1-yx9pc\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-12-46-350/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-yx9pc\",\"hosts\":[\"algo-1-yx9pc\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"16\",\"--epochs\",\"1\",\"--lr\",\"0.1\"]\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_HP_LR=0.1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m SM_HP_BATCH_SIZE=16\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m /opt/conda/bin/python3.6 train.py --batch_size 16 --epochs 1 --lr 0.1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Device Type: cuda\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m ###### Loading Cifar10 dataset\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Model network loaded from get_model_network()\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Training starts until epochs of 1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m [2021-09-27 14:12:56.625 algo-1-yx9pc:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m [2021-09-27 14:12:56.673 algo-1-yx9pc:32 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m algo-1-yx9pc:32:32 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m NCCL version 2.7.8+cuda11.1\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m [1,  2000] loss: 2.302\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m Training is finished\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m the model is saved at /opt/ml/model/model.pth\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m \n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc |\u001b[0m 2021-09-27 14:14:19,350 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mrz56mlhkp9-algo-1-yx9pc exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "local_cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='source',    \n",
    "    role=role,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters={'epochs': 1, \n",
    "                     'lr': 0.1,\n",
    "                     'batch_size': 16\n",
    "                    }                      \n",
    "    \n",
    ")\n",
    "local_cifar10_estimator.fit(local_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Host Mode 로 훈련\n",
    "- `cifar10_estimator.fit(inputs, wait=False)`\n",
    "    - 입력 데이터를 inputs로서 S3 의 경로를 제공합니다.\n",
    "    - wait=False 로 지정해서 async 모드로 훈련을 실행합니다. \n",
    "        - 실행 경과는 아래의 cifar10_estimator.logs() 에서 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " s3://sagemaker-us-east-1-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs: \\n\", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='source',    \n",
    "    role=role,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    session = sagemaker.Session(), # 세이지 메이커 세션\n",
    "    hyperparameters={'epochs': 10, \n",
    "                     'lr': 0.001,\n",
    "                     'batch_size': 16                     \n",
    "                    }                      \n",
    "    \n",
    ")\n",
    "cifar10_estimator.fit(inputs, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 14:14:28 Starting - Starting the training job...\n",
      "2021-09-27 14:14:57 Starting - Launching requested ML instancesProfilerReport-1632752068: InProgress\n",
      "......\n",
      "2021-09-27 14:15:57 Starting - Preparing the instances for training.........\n",
      "2021-09-27 14:17:17 Downloading - Downloading input data...\n",
      "2021-09-27 14:17:57 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-09-27 14:21:38,845 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-09-27 14:21:38,869 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-09-27 14:21:45,098 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-09-27 14:21:45,513 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.9.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.9.1)\u001b[0m\n",
      "\u001b[34mCollecting torchsummary==1.5.1\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.9.1->-r requirements.txt (line 4)) (8.3.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torchsummary\u001b[0m\n",
      "\u001b[34mSuccessfully installed torchsummary-1.5.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-09-27 14:21:47,871 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"lr\": 0.001,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-09-27-14-14-28-250\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-14-28-250/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":16,\"epochs\":10,\"lr\":0.001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-14-28-250/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":16,\"epochs\":10,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-09-27-14-14-28-250\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-14-28-250/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"16\",\"--epochs\",\"10\",\"--lr\",\"0.001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 16 --epochs 10 --lr 0.001\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-27 14:21:58 Training - Training image download completed. Training in progress.\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34m###### Loading Cifar10 dataset\u001b[0m\n",
      "\u001b[34mModel network loaded from get_model_network()\u001b[0m\n",
      "\u001b[34mTraining starts until epochs of 10\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.628 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.754 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.755 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.755 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.756 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:55.756 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.003 algo-1:32 INFO hook.py:591] name:module.conv1.weight count_params:450\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.003 algo-1:32 INFO hook.py:591] name:module.conv1.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.003 algo-1:32 INFO hook.py:591] name:module.conv2.weight count_params:2400\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.003 algo-1:32 INFO hook.py:591] name:module.conv2.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc1.weight count_params:48000\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc1.bias count_params:120\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc2.weight count_params:10080\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc2.bias count_params:84\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc3.weight count_params:840\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:591] name:module.fc3.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:593] Total Trainable Params: 62006\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.004 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-09-27 14:21:56.007 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\n",
      "\u001b[0m\n",
      "\u001b[34m[1,  2000] loss: 2.149\u001b[0m\n",
      "\u001b[34m[2,  2000] loss: 1.589\u001b[0m\n",
      "\u001b[34m[3,  2000] loss: 1.404\u001b[0m\n",
      "\u001b[34m[4,  2000] loss: 1.295\u001b[0m\n",
      "\u001b[34m[5,  2000] loss: 1.211\u001b[0m\n",
      "\u001b[34m[6,  2000] loss: 1.146\u001b[0m\n",
      "\u001b[34m[7,  2000] loss: 1.090\u001b[0m\n",
      "\u001b[34m[8,  2000] loss: 1.032\u001b[0m\n",
      "\u001b[34m[9,  2000] loss: 0.992\u001b[0m\n",
      "\u001b[34m[10,  2000] loss: 0.949\u001b[0m\n",
      "\u001b[34mTraining is finished\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/model.pth\u001b[0m\n",
      "\u001b[34m2021-09-27 14:24:48,420 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-09-27 14:25:01 Uploading - Uploading generated training model\n",
      "2021-09-27 14:25:01 Completed - Training job completed\n",
      "Training seconds: 465\n",
      "Billable seconds: 465\n"
     ]
    }
   ],
   "source": [
    "cifar10_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact_path:  s3://sagemaker-us-east-1-057716757052/pytorch-training-2021-09-27-14-14-28-250/output/model.tar.gz\n",
      "Stored 'artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "artifact_path = cifar10_estimator.model_data\n",
    "print(\"artifact_path: \", artifact_path)\n",
    "\n",
    "%store artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
