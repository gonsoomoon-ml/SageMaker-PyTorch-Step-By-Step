{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1.7] DDP 훈련\n",
    "\n",
    "본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 아래는 세이지메이커의 어떤 피쳐도 사용하지 않고, PyTorch 만을 사용해서 훈련 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch CIFAR-10 local training  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 inputs:  s3://sagemaker-us-east-1-057716757052/data/cifar10\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"../data\", bucket=bucket, key_prefix=\"data/cifar10\")\n",
    "print(\"s3 inputs: \", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training \n",
    "- epoch 10 , 20\n",
    "    - 각각 테스트 정확도 55.2, 62.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "# instance_type=\"ml.p3dn.24xlarge\",\n",
    "\n",
    "job_name ='cifar10-horovod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1gdw5xuaoo-algo-1-u2fhb ... \n",
      "Creating 1gdw5xuaoo-algo-1-u2fhb ... done\n",
      "Attaching to 1gdw5xuaoo-algo-1-u2fhb\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:43:18,004 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:43:18,085 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:43:18,088 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:43:18,088 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:43:18,280 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Collecting torch==1.6.0\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m \u001b[?25hCollecting torchvision==0.7.0\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Downloading torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 60.1 MB/s eta 0:00:01\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m \u001b[?25hCollecting torchsummary==1.5.1\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Collecting sagemaker_inference==1.5.5\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Downloading sagemaker_inference-1.5.5.tar.gz (20 kB)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.16.0)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (5.8.0)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: retrying==1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.3.3)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.5.4)\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Building wheels for collected packages: sagemaker-inference\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Building wheel for sagemaker-inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m \u001b[?25h  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.5.5-py2.py3-none-any.whl size=26977 sha256=b03dfd0fad91f36089ba7ac27e78aec23d485effc0bf334963de2a520ca1e1f8\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/a4/bf/81/8e084e445a44e9fbc9d64efc7afb2a660ecd06285ea4a51fa0\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Successfully built sagemaker-inference\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Installing collected packages: torch, torchvision, torchsummary, sagemaker-inference\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Attempting uninstall: torch\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m     Found existing installation: torch 1.8.1\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m     Uninstalling torch-1.8.1:\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m       Successfully uninstalled torch-1.8.1\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Rolling back uninstall of torch\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Moving to /opt/conda/bin/convert-caffe2-to-onnx\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m    from /tmp/pip-uninstall-mfbsl2eo/convert-caffe2-to-onnx\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Moving to /opt/conda/bin/convert-onnx-to-caffe2\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m    from /tmp/pip-uninstall-mfbsl2eo/convert-onnx-to-caffe2\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Moving to /opt/conda/lib/python3.6/site-packages/caffe2/\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m    from /opt/conda/lib/python3.6/site-packages/~affe2\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Moving to /opt/conda/lib/python3.6/site-packages/torch-1.8.1.dist-info/\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m    from /opt/conda/lib/python3.6/site-packages/~orch-1.8.1.dist-info\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m   Moving to /opt/conda/lib/python3.6/site-packages/torch/\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m    from /opt/conda/lib/python3.6/site-packages/~orch\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m \n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m \n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m 2021-07-09 02:44:01,226 sagemaker-training-toolkit ERROR    InstallRequirementsError:\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m Command \"/opt/conda/bin/python3.6 -m pip install -r requirements.txt\"\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb |\u001b[0m ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[36m1gdw5xuaoo-algo-1-u2fhb exited with code 1\n",
      "\u001b[0m1\n",
      "Aborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpjtkrmuww/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05a2252f2223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdebugger_hook_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \"\"\"\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpjtkrmuww/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_ddp.py\",    \n",
    "    source_dir='source',    \n",
    "    base_job_name = job_name,\n",
    "    role=role,\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py36\",\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type= instance_type,\n",
    "    # sagemaker_session=sagemaker_session,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "estimator.fit({\"training\" : inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating d0my0entk3-algo-1-2e2l1 ... \n",
      "Creating d0my0entk3-algo-1-2e2l1 ... done\n",
      "Attaching to d0my0entk3-algo-1-2e2l1\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:34:43,412 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:34:43,456 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:34:43,459 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:34:43,614 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m /opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: torchvision==0.7.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Collecting torchsummary==1.5.1\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m   Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Collecting sagemaker_inference==1.5.5\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m   Downloading sagemaker_inference-1.5.5.tar.gz (20 kB)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.19.1)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.15.0)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (5.8.0)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: retrying==1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.3.3)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sagemaker_inference==1.5.5->-r requirements.txt (line 4)) (1.5.2)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0->-r requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Building wheels for collected packages: sagemaker-inference\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m   Building wheel for sagemaker-inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \u001b[?25h  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.5.5-py2.py3-none-any.whl size=26977 sha256=2711290bbb2f0f30347c12b169760da6ed745436c2261e66a1c7a13bc8b89f3b\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/a4/bf/81/8e084e445a44e9fbc9d64efc7afb2a660ecd06285ea4a51fa0\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Successfully built sagemaker-inference\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Installing collected packages: torchsummary, sagemaker-inference\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Successfully installed sagemaker-inference-1.5.5 torchsummary-1.5.1\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:34:46,500 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Training Env:\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     },\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"current_host\": \"algo-1-2e2l1\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"algo-1-2e2l1\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     ],\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"epochs\": 20,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"lr\": 0.01,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"batch-size\": 64,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"backend\": \"gloo\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     },\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"training\": {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         }\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     },\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"job_name\": \"cifar10-horovod-2021-06-08-04-34-36-597\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"master_hostname\": \"algo-1-2e2l1\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/cifar10-horovod-2021-06-08-04-34-36-597/source/sourcedir.tar.gz\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"module_name\": \"train_horovod\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"num_gpus\": 4,\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"current_host\": \"algo-1-2e2l1\",\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m             \"algo-1-2e2l1\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m         ]\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     },\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m     \"user_entry_point\": \"train_horovod.py\"\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m }\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Environment variables:\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HOSTS=[\"algo-1-2e2l1\"]\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":20,\"lr\":0.01}\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_USER_ENTRY_POINT=train_horovod.py\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-2e2l1\",\"hosts\":[\"algo-1-2e2l1\"]}\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_CURRENT_HOST=algo-1-2e2l1\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_MODULE_NAME=train_horovod\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_NUM_GPUS=4\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/cifar10-horovod-2021-06-08-04-34-36-597/source/sourcedir.tar.gz\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-2e2l1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-2e2l1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":20,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-horovod-2021-06-08-04-34-36-597\",\"log_level\":20,\"master_hostname\":\"algo-1-2e2l1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/cifar10-horovod-2021-06-08-04-34-36-597/source/sourcedir.tar.gz\",\"module_name\":\"train_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-2e2l1\",\"hosts\":[\"algo-1-2e2l1\"]},\"user_entry_point\":\"train_horovod.py\"}\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"20\",\"--lr\",\"0.01\"]\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HP_EPOCHS=20\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HP_LR=0.01\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HP_BATCH-SIZE=64\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m SM_HP_BACKEND=gloo\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m /opt/conda/bin/python3.6 train_horovod.py --backend gloo --batch-size 64 --epochs 20 --lr 0.01\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Get train data sampler and data loader\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Get test data sampler and data loader\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Processes 50000/50000 (100%) of train data\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Processes 10000/10000 (100%) of test data\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Model loaded\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m [2021-06-08 04:34:50.276 algo-1-2e2l1:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m [2021-06-08 04:34:50.513 algo-1-2e2l1:37 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m NCCL version 2.4.8+cuda10.1\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [640/50000 (1%)] Loss: 2.294795\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [1280/50000 (3%)] Loss: 2.288875\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [1920/50000 (4%)] Loss: 2.304403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [2560/50000 (5%)] Loss: 2.301302\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [3200/50000 (6%)] Loss: 2.313438\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [3840/50000 (8%)] Loss: 2.304699\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [4480/50000 (9%)] Loss: 2.309357\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [5120/50000 (10%)] Loss: 2.308138\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [5760/50000 (12%)] Loss: 2.305488\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [6400/50000 (13%)] Loss: 2.300965\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [7040/50000 (14%)] Loss: 2.312229\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [7680/50000 (15%)] Loss: 2.313371\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [8320/50000 (17%)] Loss: 2.301967\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [8960/50000 (18%)] Loss: 2.309862\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [9600/50000 (19%)] Loss: 2.298328\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [10240/50000 (20%)] Loss: 2.308495\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [10880/50000 (22%)] Loss: 2.299371\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [11520/50000 (23%)] Loss: 2.306576\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [12160/50000 (24%)] Loss: 2.306253\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [12800/50000 (26%)] Loss: 2.301982\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [13440/50000 (27%)] Loss: 2.291117\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [14080/50000 (28%)] Loss: 2.305812\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [14720/50000 (29%)] Loss: 2.305833\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [15360/50000 (31%)] Loss: 2.299583\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [16000/50000 (32%)] Loss: 2.299588\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [16640/50000 (33%)] Loss: 2.303477\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [17280/50000 (35%)] Loss: 2.300302\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [17920/50000 (36%)] Loss: 2.298191\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [18560/50000 (37%)] Loss: 2.296181\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [19200/50000 (38%)] Loss: 2.302189\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [19840/50000 (40%)] Loss: 2.300355\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [20480/50000 (41%)] Loss: 2.300097\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [21120/50000 (42%)] Loss: 2.293346\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [21760/50000 (43%)] Loss: 2.295125\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [22400/50000 (45%)] Loss: 2.301428\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [23040/50000 (46%)] Loss: 2.300381\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [23680/50000 (47%)] Loss: 2.303212\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [24320/50000 (49%)] Loss: 2.293828\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [24960/50000 (50%)] Loss: 2.293164\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [25600/50000 (51%)] Loss: 2.298255\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [26240/50000 (52%)] Loss: 2.296529\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [26880/50000 (54%)] Loss: 2.298220\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [27520/50000 (55%)] Loss: 2.293386\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [28160/50000 (56%)] Loss: 2.297554\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [28800/50000 (58%)] Loss: 2.296219\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [29440/50000 (59%)] Loss: 2.291544\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [30080/50000 (60%)] Loss: 2.288452\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [30720/50000 (61%)] Loss: 2.284593\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [31360/50000 (63%)] Loss: 2.288181\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [32000/50000 (64%)] Loss: 2.287680\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [32640/50000 (65%)] Loss: 2.290730\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [33280/50000 (66%)] Loss: 2.292954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [33920/50000 (68%)] Loss: 2.283481\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [34560/50000 (69%)] Loss: 2.300189\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [35200/50000 (70%)] Loss: 2.281211\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [35840/50000 (72%)] Loss: 2.274063\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [36480/50000 (73%)] Loss: 2.278499\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [37120/50000 (74%)] Loss: 2.283185\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [37760/50000 (75%)] Loss: 2.282330\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [38400/50000 (77%)] Loss: 2.275517\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [39040/50000 (78%)] Loss: 2.273780\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [39680/50000 (79%)] Loss: 2.288964\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [40320/50000 (81%)] Loss: 2.256641\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [40960/50000 (82%)] Loss: 2.247279\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [41600/50000 (83%)] Loss: 2.262184\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [42240/50000 (84%)] Loss: 2.265198\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [42880/50000 (86%)] Loss: 2.296367\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [43520/50000 (87%)] Loss: 2.236273\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [44160/50000 (88%)] Loss: 2.251534\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [44800/50000 (90%)] Loss: 2.231180\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [45440/50000 (91%)] Loss: 2.247677\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [46080/50000 (92%)] Loss: 2.211856\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [46720/50000 (93%)] Loss: 2.218309\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [47360/50000 (95%)] Loss: 2.252856\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [48000/50000 (96%)] Loss: 2.220936\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [48640/50000 (97%)] Loss: 2.140243\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [49280/50000 (98%)] Loss: 2.206012\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 1 [49920/50000 (100%)] Loss: 2.237933\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 2.1792, Accuracy: 23.52%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [640/50000 (1%)] Loss: 2.128762\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [1280/50000 (3%)] Loss: 2.228696\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [1920/50000 (4%)] Loss: 2.260433\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [2560/50000 (5%)] Loss: 2.205883\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [3200/50000 (6%)] Loss: 2.190439\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [3840/50000 (8%)] Loss: 2.237749\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [4480/50000 (9%)] Loss: 2.164044\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [5120/50000 (10%)] Loss: 2.169669\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [5760/50000 (12%)] Loss: 2.293015\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [6400/50000 (13%)] Loss: 2.195561\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [7040/50000 (14%)] Loss: 2.173576\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [7680/50000 (15%)] Loss: 2.236955\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [8320/50000 (17%)] Loss: 2.156522\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [8960/50000 (18%)] Loss: 2.225260\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [9600/50000 (19%)] Loss: 2.289865\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [10240/50000 (20%)] Loss: 2.218101\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [10880/50000 (22%)] Loss: 2.219057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [11520/50000 (23%)] Loss: 2.091845\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [12160/50000 (24%)] Loss: 2.173096\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [12800/50000 (26%)] Loss: 2.149865\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [13440/50000 (27%)] Loss: 2.223381\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [14080/50000 (28%)] Loss: 2.118982\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [14720/50000 (29%)] Loss: 2.018418\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [15360/50000 (31%)] Loss: 2.163658\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [16000/50000 (32%)] Loss: 2.055994\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [16640/50000 (33%)] Loss: 2.410770\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [17280/50000 (35%)] Loss: 2.158491\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [17920/50000 (36%)] Loss: 2.143955\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [18560/50000 (37%)] Loss: 2.128340\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [19200/50000 (38%)] Loss: 2.251614\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [19840/50000 (40%)] Loss: 2.197502\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [20480/50000 (41%)] Loss: 2.232788\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [21120/50000 (42%)] Loss: 2.180450\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [21760/50000 (43%)] Loss: 2.132370\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [22400/50000 (45%)] Loss: 2.220330\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [23040/50000 (46%)] Loss: 2.130466\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [23680/50000 (47%)] Loss: 2.179449\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [24320/50000 (49%)] Loss: 1.997389\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [24960/50000 (50%)] Loss: 2.126515\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [25600/50000 (51%)] Loss: 2.170513\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [26240/50000 (52%)] Loss: 2.160557\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [26880/50000 (54%)] Loss: 2.145296\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [27520/50000 (55%)] Loss: 2.082998\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [28160/50000 (56%)] Loss: 2.165444\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [28800/50000 (58%)] Loss: 2.129939\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [29440/50000 (59%)] Loss: 2.018520\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [30080/50000 (60%)] Loss: 2.070299\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [30720/50000 (61%)] Loss: 2.119917\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [31360/50000 (63%)] Loss: 2.022848\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [32000/50000 (64%)] Loss: 2.159747\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [32640/50000 (65%)] Loss: 2.167368\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [33280/50000 (66%)] Loss: 2.054576\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [33920/50000 (68%)] Loss: 2.073268\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [34560/50000 (69%)] Loss: 2.058966\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [35200/50000 (70%)] Loss: 2.133671\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [35840/50000 (72%)] Loss: 2.113641\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [36480/50000 (73%)] Loss: 2.081857\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [37120/50000 (74%)] Loss: 2.079351\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [37760/50000 (75%)] Loss: 1.983304\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [38400/50000 (77%)] Loss: 2.143410\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [39040/50000 (78%)] Loss: 2.175790\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [39680/50000 (79%)] Loss: 2.016187\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [40320/50000 (81%)] Loss: 2.084416\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [40960/50000 (82%)] Loss: 2.092233\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [41600/50000 (83%)] Loss: 2.078083\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [42240/50000 (84%)] Loss: 2.093395\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [42880/50000 (86%)] Loss: 2.026649\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [43520/50000 (87%)] Loss: 2.106950\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [44160/50000 (88%)] Loss: 2.134976\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [44800/50000 (90%)] Loss: 2.085214\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [45440/50000 (91%)] Loss: 2.236590\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [46080/50000 (92%)] Loss: 2.139433\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [46720/50000 (93%)] Loss: 2.043864\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [47360/50000 (95%)] Loss: 2.124044\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [48000/50000 (96%)] Loss: 1.982738\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [48640/50000 (97%)] Loss: 2.100720\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [49280/50000 (98%)] Loss: 2.045613\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 2 [49920/50000 (100%)] Loss: 2.067956\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 2.0004, Accuracy: 32.13%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [640/50000 (1%)] Loss: 1.989216\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [1280/50000 (3%)] Loss: 2.061764\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [1920/50000 (4%)] Loss: 2.015318\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [2560/50000 (5%)] Loss: 2.121759\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [3200/50000 (6%)] Loss: 2.116300\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [3840/50000 (8%)] Loss: 2.131956\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [4480/50000 (9%)] Loss: 2.042589\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [5120/50000 (10%)] Loss: 1.984523\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [5760/50000 (12%)] Loss: 2.057543\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [6400/50000 (13%)] Loss: 2.079462\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [7040/50000 (14%)] Loss: 2.079662\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [7680/50000 (15%)] Loss: 2.200132\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [8320/50000 (17%)] Loss: 2.046892\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [8960/50000 (18%)] Loss: 1.989443\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [9600/50000 (19%)] Loss: 2.171811\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [10240/50000 (20%)] Loss: 2.055488\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [10880/50000 (22%)] Loss: 2.164160\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [11520/50000 (23%)] Loss: 2.091776\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [12160/50000 (24%)] Loss: 2.058897\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [12800/50000 (26%)] Loss: 2.002544\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [13440/50000 (27%)] Loss: 2.036645\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [14080/50000 (28%)] Loss: 2.114545\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [14720/50000 (29%)] Loss: 1.863185\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [15360/50000 (31%)] Loss: 2.057033\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [16000/50000 (32%)] Loss: 2.081637\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [16640/50000 (33%)] Loss: 2.151787\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [17280/50000 (35%)] Loss: 2.231822\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [17920/50000 (36%)] Loss: 2.017002\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [18560/50000 (37%)] Loss: 2.087049\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [19200/50000 (38%)] Loss: 1.961905\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [19840/50000 (40%)] Loss: 1.992661\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [20480/50000 (41%)] Loss: 2.114325\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [21120/50000 (42%)] Loss: 2.190480\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [21760/50000 (43%)] Loss: 1.969623\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [22400/50000 (45%)] Loss: 2.142831\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [23040/50000 (46%)] Loss: 2.049716\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [23680/50000 (47%)] Loss: 2.085823\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [24320/50000 (49%)] Loss: 2.103420\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [24960/50000 (50%)] Loss: 2.074767\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [25600/50000 (51%)] Loss: 2.071231\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [26240/50000 (52%)] Loss: 2.091569\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [26880/50000 (54%)] Loss: 1.996197\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [27520/50000 (55%)] Loss: 2.168161\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [28160/50000 (56%)] Loss: 1.831916\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [28800/50000 (58%)] Loss: 2.073150\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [29440/50000 (59%)] Loss: 1.835808\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [30080/50000 (60%)] Loss: 2.040442\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [30720/50000 (61%)] Loss: 2.020645\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [31360/50000 (63%)] Loss: 2.062991\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [32000/50000 (64%)] Loss: 2.058079\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [32640/50000 (65%)] Loss: 2.047973\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [33280/50000 (66%)] Loss: 2.052381\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [33920/50000 (68%)] Loss: 1.963799\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [34560/50000 (69%)] Loss: 2.065867\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [35200/50000 (70%)] Loss: 1.961997\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [35840/50000 (72%)] Loss: 1.935377\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [36480/50000 (73%)] Loss: 1.909329\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [37120/50000 (74%)] Loss: 1.949302\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [37760/50000 (75%)] Loss: 1.985268\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [38400/50000 (77%)] Loss: 2.008682\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [39040/50000 (78%)] Loss: 2.023616\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [39680/50000 (79%)] Loss: 1.917379\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [40320/50000 (81%)] Loss: 1.983955\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [40960/50000 (82%)] Loss: 2.069860\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [41600/50000 (83%)] Loss: 2.047031\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [42240/50000 (84%)] Loss: 1.957482\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [42880/50000 (86%)] Loss: 1.995692\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [43520/50000 (87%)] Loss: 2.011992\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [44160/50000 (88%)] Loss: 1.931452\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [44800/50000 (90%)] Loss: 1.843672\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [45440/50000 (91%)] Loss: 2.038741\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [46080/50000 (92%)] Loss: 2.024475\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [46720/50000 (93%)] Loss: 1.956854\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [47360/50000 (95%)] Loss: 2.018902\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [48000/50000 (96%)] Loss: 1.900608\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [48640/50000 (97%)] Loss: 1.896253\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [49280/50000 (98%)] Loss: 1.955195\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 3 [49920/50000 (100%)] Loss: 2.160164\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.8507, Accuracy: 37.96%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [640/50000 (1%)] Loss: 1.909974\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [1280/50000 (3%)] Loss: 1.788186\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [1920/50000 (4%)] Loss: 2.182740\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [2560/50000 (5%)] Loss: 1.953406\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [3200/50000 (6%)] Loss: 2.014198\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [3840/50000 (8%)] Loss: 1.916425\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [4480/50000 (9%)] Loss: 1.812281\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [5120/50000 (10%)] Loss: 2.086316\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [5760/50000 (12%)] Loss: 2.083981\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [6400/50000 (13%)] Loss: 1.813715\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [7040/50000 (14%)] Loss: 2.082030\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [7680/50000 (15%)] Loss: 2.030209\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [8320/50000 (17%)] Loss: 1.839372\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [8960/50000 (18%)] Loss: 1.846337\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [9600/50000 (19%)] Loss: 2.019821\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [10240/50000 (20%)] Loss: 1.925526\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [10880/50000 (22%)] Loss: 1.928364\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [11520/50000 (23%)] Loss: 1.917887\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [12160/50000 (24%)] Loss: 1.972891\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [12800/50000 (26%)] Loss: 1.760396\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [13440/50000 (27%)] Loss: 1.942748\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [14080/50000 (28%)] Loss: 1.821892\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [14720/50000 (29%)] Loss: 1.981232\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [15360/50000 (31%)] Loss: 1.894490\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [16000/50000 (32%)] Loss: 1.857321\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [16640/50000 (33%)] Loss: 2.045734\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [17280/50000 (35%)] Loss: 2.083514\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [17920/50000 (36%)] Loss: 1.973795\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [18560/50000 (37%)] Loss: 2.006720\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [19200/50000 (38%)] Loss: 2.039625\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [19840/50000 (40%)] Loss: 2.040738\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [20480/50000 (41%)] Loss: 2.064983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [21120/50000 (42%)] Loss: 2.026119\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [21760/50000 (43%)] Loss: 1.712096\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [22400/50000 (45%)] Loss: 2.235888\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [23040/50000 (46%)] Loss: 1.990778\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [23680/50000 (47%)] Loss: 1.956086\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [24320/50000 (49%)] Loss: 1.955280\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [24960/50000 (50%)] Loss: 2.011542\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [25600/50000 (51%)] Loss: 1.929260\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [26240/50000 (52%)] Loss: 1.927821\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [26880/50000 (54%)] Loss: 1.956417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [27520/50000 (55%)] Loss: 1.913954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [28160/50000 (56%)] Loss: 1.865316\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [28800/50000 (58%)] Loss: 1.837913\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [29440/50000 (59%)] Loss: 1.920994\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [30080/50000 (60%)] Loss: 1.883965\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [30720/50000 (61%)] Loss: 1.938294\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [31360/50000 (63%)] Loss: 1.792904\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [32000/50000 (64%)] Loss: 1.824176\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [32640/50000 (65%)] Loss: 2.026637\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [33280/50000 (66%)] Loss: 2.030620\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [33920/50000 (68%)] Loss: 1.836285\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [34560/50000 (69%)] Loss: 1.900979\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [35200/50000 (70%)] Loss: 1.852407\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [35840/50000 (72%)] Loss: 1.747029\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [36480/50000 (73%)] Loss: 1.996120\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [37120/50000 (74%)] Loss: 1.886362\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [37760/50000 (75%)] Loss: 1.799426\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [38400/50000 (77%)] Loss: 1.898210\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [39040/50000 (78%)] Loss: 1.910601\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [39680/50000 (79%)] Loss: 1.821892\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [40320/50000 (81%)] Loss: 1.926428\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [40960/50000 (82%)] Loss: 1.981052\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [41600/50000 (83%)] Loss: 1.864123\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [42240/50000 (84%)] Loss: 1.974732\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [42880/50000 (86%)] Loss: 1.955206\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [43520/50000 (87%)] Loss: 2.023280\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [44160/50000 (88%)] Loss: 1.815892\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [44800/50000 (90%)] Loss: 1.820377\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [45440/50000 (91%)] Loss: 2.077414\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [46080/50000 (92%)] Loss: 1.920611\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [46720/50000 (93%)] Loss: 1.870621\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [47360/50000 (95%)] Loss: 1.890578\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [48000/50000 (96%)] Loss: 1.829882\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [48640/50000 (97%)] Loss: 1.869470\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [49280/50000 (98%)] Loss: 1.824087\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 4 [49920/50000 (100%)] Loss: 1.916138\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.6926, Accuracy: 44.54%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [640/50000 (1%)] Loss: 1.836791\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [1280/50000 (3%)] Loss: 1.822850\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [1920/50000 (4%)] Loss: 2.000654\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [2560/50000 (5%)] Loss: 1.936783\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [3200/50000 (6%)] Loss: 1.994393\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [3840/50000 (8%)] Loss: 1.924002\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [4480/50000 (9%)] Loss: 1.851158\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [5120/50000 (10%)] Loss: 1.836948\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [5760/50000 (12%)] Loss: 1.913519\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [6400/50000 (13%)] Loss: 1.775342\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [7040/50000 (14%)] Loss: 1.897743\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [7680/50000 (15%)] Loss: 1.920610\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [8320/50000 (17%)] Loss: 1.970219\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [8960/50000 (18%)] Loss: 1.899068\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [9600/50000 (19%)] Loss: 1.954683\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [10240/50000 (20%)] Loss: 1.977277\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [10880/50000 (22%)] Loss: 1.962823\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [11520/50000 (23%)] Loss: 1.937487\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [12160/50000 (24%)] Loss: 1.929418\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [12800/50000 (26%)] Loss: 1.710570\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [13440/50000 (27%)] Loss: 1.992710\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [14080/50000 (28%)] Loss: 1.836890\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [14720/50000 (29%)] Loss: 1.742739\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [15360/50000 (31%)] Loss: 1.920659\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [16000/50000 (32%)] Loss: 1.883373\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [16640/50000 (33%)] Loss: 1.975716\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [17280/50000 (35%)] Loss: 1.851576\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [17920/50000 (36%)] Loss: 1.835298\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [18560/50000 (37%)] Loss: 1.967655\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [19200/50000 (38%)] Loss: 1.955291\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [19840/50000 (40%)] Loss: 2.062628\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [20480/50000 (41%)] Loss: 1.800336\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [21120/50000 (42%)] Loss: 1.963254\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [21760/50000 (43%)] Loss: 1.765794\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [22400/50000 (45%)] Loss: 2.013462\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [23040/50000 (46%)] Loss: 1.932665\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [23680/50000 (47%)] Loss: 1.900977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [24320/50000 (49%)] Loss: 1.868984\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [24960/50000 (50%)] Loss: 2.043930\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [25600/50000 (51%)] Loss: 1.834027\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [26240/50000 (52%)] Loss: 1.923359\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [26880/50000 (54%)] Loss: 1.758519\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [27520/50000 (55%)] Loss: 1.764071\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [28160/50000 (56%)] Loss: 1.866155\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [28800/50000 (58%)] Loss: 1.983479\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [29440/50000 (59%)] Loss: 1.881953\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [30080/50000 (60%)] Loss: 1.921769\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [30720/50000 (61%)] Loss: 1.920466\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [31360/50000 (63%)] Loss: 1.888705\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [32000/50000 (64%)] Loss: 1.747023\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [32640/50000 (65%)] Loss: 1.982323\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [33280/50000 (66%)] Loss: 1.794514\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [33920/50000 (68%)] Loss: 1.871478\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [34560/50000 (69%)] Loss: 1.925084\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [35200/50000 (70%)] Loss: 1.840055\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [35840/50000 (72%)] Loss: 1.774518\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [36480/50000 (73%)] Loss: 1.793212\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [37120/50000 (74%)] Loss: 1.886144\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [37760/50000 (75%)] Loss: 1.830768\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [38400/50000 (77%)] Loss: 1.751610\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [39040/50000 (78%)] Loss: 1.719440\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [39680/50000 (79%)] Loss: 1.826315\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [40320/50000 (81%)] Loss: 1.875657\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [40960/50000 (82%)] Loss: 1.826002\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [41600/50000 (83%)] Loss: 1.650683\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [42240/50000 (84%)] Loss: 1.990740\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [42880/50000 (86%)] Loss: 1.838251\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [43520/50000 (87%)] Loss: 1.959738\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [44160/50000 (88%)] Loss: 1.870338\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [44800/50000 (90%)] Loss: 1.873223\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [45440/50000 (91%)] Loss: 1.968468\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [46080/50000 (92%)] Loss: 1.872268\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [46720/50000 (93%)] Loss: 1.709987\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [47360/50000 (95%)] Loss: 1.845870\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [48000/50000 (96%)] Loss: 1.862718\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [48640/50000 (97%)] Loss: 1.842784\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [49280/50000 (98%)] Loss: 1.905014\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 5 [49920/50000 (100%)] Loss: 1.865992\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.6371, Accuracy: 47.32%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [640/50000 (1%)] Loss: 1.727389\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [1280/50000 (3%)] Loss: 1.703655\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [1920/50000 (4%)] Loss: 1.856921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [2560/50000 (5%)] Loss: 1.885341\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [3200/50000 (6%)] Loss: 1.905351\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [3840/50000 (8%)] Loss: 1.936933\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [4480/50000 (9%)] Loss: 1.691360\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [5120/50000 (10%)] Loss: 1.886850\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [5760/50000 (12%)] Loss: 1.877519\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [6400/50000 (13%)] Loss: 1.704921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [7040/50000 (14%)] Loss: 1.959782\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [7680/50000 (15%)] Loss: 1.907134\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [8320/50000 (17%)] Loss: 1.826477\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [8960/50000 (18%)] Loss: 1.905064\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [9600/50000 (19%)] Loss: 1.905756\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [10240/50000 (20%)] Loss: 1.876321\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [10880/50000 (22%)] Loss: 1.769954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [11520/50000 (23%)] Loss: 1.908949\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [12160/50000 (24%)] Loss: 1.777718\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [12800/50000 (26%)] Loss: 1.662291\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [13440/50000 (27%)] Loss: 1.899924\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [14080/50000 (28%)] Loss: 1.950386\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [14720/50000 (29%)] Loss: 1.707493\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [15360/50000 (31%)] Loss: 1.676741\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [16000/50000 (32%)] Loss: 1.806696\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [16640/50000 (33%)] Loss: 1.775270\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [17280/50000 (35%)] Loss: 1.823846\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [17920/50000 (36%)] Loss: 1.706969\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [18560/50000 (37%)] Loss: 2.008561\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [19200/50000 (38%)] Loss: 1.902696\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [19840/50000 (40%)] Loss: 1.747714\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [20480/50000 (41%)] Loss: 1.851667\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [21120/50000 (42%)] Loss: 1.906342\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [21760/50000 (43%)] Loss: 1.584992\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [22400/50000 (45%)] Loss: 1.861005\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [23040/50000 (46%)] Loss: 1.878397\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [23680/50000 (47%)] Loss: 1.862564\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [24320/50000 (49%)] Loss: 1.802294\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [24960/50000 (50%)] Loss: 1.896849\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [25600/50000 (51%)] Loss: 1.686770\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [26240/50000 (52%)] Loss: 1.861024\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [26880/50000 (54%)] Loss: 1.887886\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [27520/50000 (55%)] Loss: 1.786608\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [28160/50000 (56%)] Loss: 1.924921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [28800/50000 (58%)] Loss: 1.716307\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [29440/50000 (59%)] Loss: 1.701879\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [30080/50000 (60%)] Loss: 1.928251\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [30720/50000 (61%)] Loss: 1.967392\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [31360/50000 (63%)] Loss: 1.703824\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [32000/50000 (64%)] Loss: 1.771982\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [32640/50000 (65%)] Loss: 1.909972\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [33280/50000 (66%)] Loss: 1.886668\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [33920/50000 (68%)] Loss: 1.638119\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [34560/50000 (69%)] Loss: 1.913736\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [35200/50000 (70%)] Loss: 1.646856\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [35840/50000 (72%)] Loss: 1.779834\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [36480/50000 (73%)] Loss: 1.864973\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [37120/50000 (74%)] Loss: 1.797889\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [37760/50000 (75%)] Loss: 1.770125\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [38400/50000 (77%)] Loss: 1.794585\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [39040/50000 (78%)] Loss: 1.684655\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [39680/50000 (79%)] Loss: 1.802789\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [40320/50000 (81%)] Loss: 1.693239\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [40960/50000 (82%)] Loss: 1.757908\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [41600/50000 (83%)] Loss: 1.696850\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [42240/50000 (84%)] Loss: 1.864218\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [42880/50000 (86%)] Loss: 1.763006\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [43520/50000 (87%)] Loss: 1.800672\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [44160/50000 (88%)] Loss: 1.678257\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [44800/50000 (90%)] Loss: 1.854940\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [45440/50000 (91%)] Loss: 1.898431\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [46080/50000 (92%)] Loss: 1.834688\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [46720/50000 (93%)] Loss: 1.639153\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [47360/50000 (95%)] Loss: 1.676931\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [48000/50000 (96%)] Loss: 1.697140\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [48640/50000 (97%)] Loss: 1.717168\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [49280/50000 (98%)] Loss: 1.882964\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 6 [49920/50000 (100%)] Loss: 1.800663\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.5625, Accuracy: 49.95%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [640/50000 (1%)] Loss: 1.768359\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [1280/50000 (3%)] Loss: 1.804017\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [1920/50000 (4%)] Loss: 1.932021\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [2560/50000 (5%)] Loss: 1.899403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [3200/50000 (6%)] Loss: 1.800755\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [3840/50000 (8%)] Loss: 1.914973\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [4480/50000 (9%)] Loss: 1.671186\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [5120/50000 (10%)] Loss: 1.693570\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [5760/50000 (12%)] Loss: 1.900472\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [6400/50000 (13%)] Loss: 1.711743\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [7040/50000 (14%)] Loss: 1.779658\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [7680/50000 (15%)] Loss: 1.896070\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [8320/50000 (17%)] Loss: 1.841932\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [8960/50000 (18%)] Loss: 1.890429\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [9600/50000 (19%)] Loss: 1.900374\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [10240/50000 (20%)] Loss: 1.770128\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [10880/50000 (22%)] Loss: 1.765439\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [11520/50000 (23%)] Loss: 1.874243\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [12160/50000 (24%)] Loss: 1.868307\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [12800/50000 (26%)] Loss: 1.668981\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [13440/50000 (27%)] Loss: 1.794329\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [14080/50000 (28%)] Loss: 1.853837\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [14720/50000 (29%)] Loss: 1.644057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [15360/50000 (31%)] Loss: 1.862376\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [16000/50000 (32%)] Loss: 1.770757\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [16640/50000 (33%)] Loss: 1.721332\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [17280/50000 (35%)] Loss: 1.565207\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [17920/50000 (36%)] Loss: 1.977213\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [18560/50000 (37%)] Loss: 1.872215\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [19200/50000 (38%)] Loss: 1.837883\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [19840/50000 (40%)] Loss: 1.906625\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [20480/50000 (41%)] Loss: 1.877677\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [21120/50000 (42%)] Loss: 1.755879\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [21760/50000 (43%)] Loss: 1.659256\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [22400/50000 (45%)] Loss: 1.942906\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [23040/50000 (46%)] Loss: 1.905131\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [23680/50000 (47%)] Loss: 1.825473\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [24320/50000 (49%)] Loss: 1.625400\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [24960/50000 (50%)] Loss: 1.831479\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [25600/50000 (51%)] Loss: 1.824007\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [26240/50000 (52%)] Loss: 1.936957\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [26880/50000 (54%)] Loss: 1.668923\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [27520/50000 (55%)] Loss: 1.747068\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [28160/50000 (56%)] Loss: 1.833897\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [28800/50000 (58%)] Loss: 1.824522\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [29440/50000 (59%)] Loss: 1.962819\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [30080/50000 (60%)] Loss: 1.864730\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [30720/50000 (61%)] Loss: 1.791209\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [31360/50000 (63%)] Loss: 1.644517\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [32000/50000 (64%)] Loss: 1.882447\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [32640/50000 (65%)] Loss: 1.851716\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [33280/50000 (66%)] Loss: 1.795880\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [33920/50000 (68%)] Loss: 1.562975\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [34560/50000 (69%)] Loss: 1.795727\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [35200/50000 (70%)] Loss: 1.662402\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [35840/50000 (72%)] Loss: 1.827714\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [36480/50000 (73%)] Loss: 1.840560\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [37120/50000 (74%)] Loss: 1.851869\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [37760/50000 (75%)] Loss: 1.689891\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [38400/50000 (77%)] Loss: 1.848699\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [39040/50000 (78%)] Loss: 1.867137\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [39680/50000 (79%)] Loss: 1.668183\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [40320/50000 (81%)] Loss: 1.722557\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [40960/50000 (82%)] Loss: 1.889432\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [41600/50000 (83%)] Loss: 1.613762\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [42240/50000 (84%)] Loss: 1.810708\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [42880/50000 (86%)] Loss: 1.899928\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [43520/50000 (87%)] Loss: 1.681648\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [44160/50000 (88%)] Loss: 1.597614\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [44800/50000 (90%)] Loss: 1.791602\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [45440/50000 (91%)] Loss: 1.880926\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [46080/50000 (92%)] Loss: 1.826078\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [46720/50000 (93%)] Loss: 1.701223\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [47360/50000 (95%)] Loss: 1.871153\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [48000/50000 (96%)] Loss: 1.666124\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [48640/50000 (97%)] Loss: 1.658774\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [49280/50000 (98%)] Loss: 1.736434\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 7 [49920/50000 (100%)] Loss: 1.747165\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.5033, Accuracy: 51.29%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [640/50000 (1%)] Loss: 1.753060\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [1280/50000 (3%)] Loss: 1.791577\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [1920/50000 (4%)] Loss: 1.644498\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [2560/50000 (5%)] Loss: 1.736374\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [3200/50000 (6%)] Loss: 1.797522\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [3840/50000 (8%)] Loss: 1.756470\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [4480/50000 (9%)] Loss: 1.637504\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [5120/50000 (10%)] Loss: 1.776235\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [5760/50000 (12%)] Loss: 1.816182\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [6400/50000 (13%)] Loss: 1.717895\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [7040/50000 (14%)] Loss: 1.807859\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [7680/50000 (15%)] Loss: 1.921441\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [8320/50000 (17%)] Loss: 1.783672\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [8960/50000 (18%)] Loss: 1.698160\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [9600/50000 (19%)] Loss: 1.931159\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [10240/50000 (20%)] Loss: 1.609180\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [10880/50000 (22%)] Loss: 1.746111\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [11520/50000 (23%)] Loss: 1.771554\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [12160/50000 (24%)] Loss: 1.732184\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [12800/50000 (26%)] Loss: 1.678977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [13440/50000 (27%)] Loss: 1.955853\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [14080/50000 (28%)] Loss: 1.752057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [14720/50000 (29%)] Loss: 1.722611\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [15360/50000 (31%)] Loss: 1.869119\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [16000/50000 (32%)] Loss: 1.881026\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [16640/50000 (33%)] Loss: 1.666632\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [17280/50000 (35%)] Loss: 1.618181\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [17920/50000 (36%)] Loss: 1.759688\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [18560/50000 (37%)] Loss: 1.860597\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [19200/50000 (38%)] Loss: 1.892832\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [19840/50000 (40%)] Loss: 1.787458\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [20480/50000 (41%)] Loss: 1.783156\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [21120/50000 (42%)] Loss: 1.941836\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [21760/50000 (43%)] Loss: 1.531806\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [22400/50000 (45%)] Loss: 1.831890\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [23040/50000 (46%)] Loss: 1.805575\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [23680/50000 (47%)] Loss: 1.845391\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [24320/50000 (49%)] Loss: 1.784298\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [24960/50000 (50%)] Loss: 1.736835\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [25600/50000 (51%)] Loss: 1.627186\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [26240/50000 (52%)] Loss: 1.817933\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [26880/50000 (54%)] Loss: 1.737792\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [27520/50000 (55%)] Loss: 1.719436\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [28160/50000 (56%)] Loss: 1.816173\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [28800/50000 (58%)] Loss: 1.838798\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [29440/50000 (59%)] Loss: 1.803824\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [30080/50000 (60%)] Loss: 1.825481\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [30720/50000 (61%)] Loss: 1.915280\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [31360/50000 (63%)] Loss: 1.659439\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [32000/50000 (64%)] Loss: 1.811243\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [32640/50000 (65%)] Loss: 1.842803\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [33280/50000 (66%)] Loss: 1.834742\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [33920/50000 (68%)] Loss: 1.695161\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [34560/50000 (69%)] Loss: 1.759244\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [35200/50000 (70%)] Loss: 1.795356\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [35840/50000 (72%)] Loss: 1.629386\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [36480/50000 (73%)] Loss: 1.642098\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [37120/50000 (74%)] Loss: 1.762710\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [37760/50000 (75%)] Loss: 1.580598\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [38400/50000 (77%)] Loss: 1.545463\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [39040/50000 (78%)] Loss: 1.546368\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [39680/50000 (79%)] Loss: 1.634671\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [40320/50000 (81%)] Loss: 1.515398\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [40960/50000 (82%)] Loss: 1.733131\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [41600/50000 (83%)] Loss: 1.640483\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [42240/50000 (84%)] Loss: 1.652027\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [42880/50000 (86%)] Loss: 1.777442\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [43520/50000 (87%)] Loss: 1.863528\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [44160/50000 (88%)] Loss: 1.653305\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [44800/50000 (90%)] Loss: 1.892711\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [45440/50000 (91%)] Loss: 1.977663\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [46080/50000 (92%)] Loss: 1.731624\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [46720/50000 (93%)] Loss: 1.699986\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [47360/50000 (95%)] Loss: 1.834697\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [48000/50000 (96%)] Loss: 1.734233\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [48640/50000 (97%)] Loss: 1.806257\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [49280/50000 (98%)] Loss: 1.643826\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 8 [49920/50000 (100%)] Loss: 1.794628\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.4637, Accuracy: 53.20%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [640/50000 (1%)] Loss: 1.541497\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [1280/50000 (3%)] Loss: 1.755628\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [1920/50000 (4%)] Loss: 1.866498\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [2560/50000 (5%)] Loss: 1.877696\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [3200/50000 (6%)] Loss: 1.453417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [3840/50000 (8%)] Loss: 1.865605\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [4480/50000 (9%)] Loss: 1.647769\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [5120/50000 (10%)] Loss: 1.706863\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [5760/50000 (12%)] Loss: 1.959462\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [6400/50000 (13%)] Loss: 1.531687\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [7040/50000 (14%)] Loss: 1.705747\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [7680/50000 (15%)] Loss: 1.825423\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [8320/50000 (17%)] Loss: 1.869442\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [8960/50000 (18%)] Loss: 1.857998\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [9600/50000 (19%)] Loss: 1.912183\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [10240/50000 (20%)] Loss: 1.488971\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [10880/50000 (22%)] Loss: 1.512832\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [11520/50000 (23%)] Loss: 1.569121\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [12160/50000 (24%)] Loss: 1.766718\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [12800/50000 (26%)] Loss: 1.549261\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [13440/50000 (27%)] Loss: 1.890327\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [14080/50000 (28%)] Loss: 1.804417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [14720/50000 (29%)] Loss: 1.529689\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [15360/50000 (31%)] Loss: 1.887468\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [16000/50000 (32%)] Loss: 1.786124\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [16640/50000 (33%)] Loss: 1.753621\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [17280/50000 (35%)] Loss: 1.744338\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [17920/50000 (36%)] Loss: 1.887288\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [18560/50000 (37%)] Loss: 1.707199\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [19200/50000 (38%)] Loss: 1.889653\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [19840/50000 (40%)] Loss: 1.847755\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [20480/50000 (41%)] Loss: 1.714700\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [21120/50000 (42%)] Loss: 1.708659\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [21760/50000 (43%)] Loss: 1.548387\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [22400/50000 (45%)] Loss: 1.941149\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [23040/50000 (46%)] Loss: 1.839419\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [23680/50000 (47%)] Loss: 1.783051\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [24320/50000 (49%)] Loss: 1.865348\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [24960/50000 (50%)] Loss: 1.805853\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [25600/50000 (51%)] Loss: 1.775969\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [26240/50000 (52%)] Loss: 1.696773\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [26880/50000 (54%)] Loss: 1.703809\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [27520/50000 (55%)] Loss: 1.590127\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [28160/50000 (56%)] Loss: 1.601868\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [28800/50000 (58%)] Loss: 1.808266\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [29440/50000 (59%)] Loss: 1.789376\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [30080/50000 (60%)] Loss: 1.810052\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [30720/50000 (61%)] Loss: 1.754166\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [31360/50000 (63%)] Loss: 1.502078\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [32000/50000 (64%)] Loss: 1.771983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [32640/50000 (65%)] Loss: 1.682393\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [33280/50000 (66%)] Loss: 1.714977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [33920/50000 (68%)] Loss: 1.578849\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [34560/50000 (69%)] Loss: 1.742612\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [35200/50000 (70%)] Loss: 1.601398\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [35840/50000 (72%)] Loss: 1.661587\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [36480/50000 (73%)] Loss: 1.770009\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [37120/50000 (74%)] Loss: 1.683507\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [37760/50000 (75%)] Loss: 1.493601\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [38400/50000 (77%)] Loss: 1.727215\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [39040/50000 (78%)] Loss: 1.586628\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [39680/50000 (79%)] Loss: 1.712907\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [40320/50000 (81%)] Loss: 1.728074\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [40960/50000 (82%)] Loss: 1.689671\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [41600/50000 (83%)] Loss: 1.598706\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [42240/50000 (84%)] Loss: 1.721748\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [42880/50000 (86%)] Loss: 1.817415\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [43520/50000 (87%)] Loss: 1.894919\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [44160/50000 (88%)] Loss: 1.670215\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [44800/50000 (90%)] Loss: 1.598310\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [45440/50000 (91%)] Loss: 1.875707\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [46080/50000 (92%)] Loss: 1.788727\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [46720/50000 (93%)] Loss: 1.622691\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [47360/50000 (95%)] Loss: 1.697928\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [48000/50000 (96%)] Loss: 1.591545\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [48640/50000 (97%)] Loss: 1.703740\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [49280/50000 (98%)] Loss: 1.831126\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 9 [49920/50000 (100%)] Loss: 1.687446\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.4387, Accuracy: 53.87%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [640/50000 (1%)] Loss: 1.523702\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [1280/50000 (3%)] Loss: 1.563685\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [1920/50000 (4%)] Loss: 1.787623\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [2560/50000 (5%)] Loss: 1.712792\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [3200/50000 (6%)] Loss: 1.838565\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [3840/50000 (8%)] Loss: 1.723868\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [4480/50000 (9%)] Loss: 1.633283\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [5120/50000 (10%)] Loss: 1.664127\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [5760/50000 (12%)] Loss: 1.676450\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [6400/50000 (13%)] Loss: 1.574603\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [7040/50000 (14%)] Loss: 1.592614\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [7680/50000 (15%)] Loss: 1.854072\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [8320/50000 (17%)] Loss: 1.635408\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [8960/50000 (18%)] Loss: 1.794167\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [9600/50000 (19%)] Loss: 1.720921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [10240/50000 (20%)] Loss: 1.727610\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [10880/50000 (22%)] Loss: 1.760453\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [11520/50000 (23%)] Loss: 1.649964\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [12160/50000 (24%)] Loss: 1.610360\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [12800/50000 (26%)] Loss: 1.522746\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [13440/50000 (27%)] Loss: 1.770980\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [14080/50000 (28%)] Loss: 1.836173\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [14720/50000 (29%)] Loss: 1.531000\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [15360/50000 (31%)] Loss: 1.763199\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [16000/50000 (32%)] Loss: 1.709916\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [16640/50000 (33%)] Loss: 1.658780\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [17280/50000 (35%)] Loss: 1.743183\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [17920/50000 (36%)] Loss: 1.741170\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [18560/50000 (37%)] Loss: 1.823958\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [19200/50000 (38%)] Loss: 1.990677\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [19840/50000 (40%)] Loss: 1.847045\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [20480/50000 (41%)] Loss: 1.683406\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [21120/50000 (42%)] Loss: 1.769304\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [21760/50000 (43%)] Loss: 1.672486\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [22400/50000 (45%)] Loss: 1.917101\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [23040/50000 (46%)] Loss: 1.680836\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [23680/50000 (47%)] Loss: 1.735045\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [24320/50000 (49%)] Loss: 1.663576\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [24960/50000 (50%)] Loss: 1.763118\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [25600/50000 (51%)] Loss: 1.588667\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [26240/50000 (52%)] Loss: 1.736891\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [26880/50000 (54%)] Loss: 1.635109\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [27520/50000 (55%)] Loss: 1.587344\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [28160/50000 (56%)] Loss: 1.612697\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [28800/50000 (58%)] Loss: 1.879828\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [29440/50000 (59%)] Loss: 1.750983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [30080/50000 (60%)] Loss: 1.541942\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [30720/50000 (61%)] Loss: 1.682538\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [31360/50000 (63%)] Loss: 1.625714\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [32000/50000 (64%)] Loss: 1.584445\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [32640/50000 (65%)] Loss: 1.755469\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [33280/50000 (66%)] Loss: 1.940889\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [33920/50000 (68%)] Loss: 1.457821\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [34560/50000 (69%)] Loss: 1.638193\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [35200/50000 (70%)] Loss: 1.632972\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [35840/50000 (72%)] Loss: 1.652083\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [36480/50000 (73%)] Loss: 1.618837\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [37120/50000 (74%)] Loss: 1.873109\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [37760/50000 (75%)] Loss: 1.584198\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [38400/50000 (77%)] Loss: 1.728158\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [39040/50000 (78%)] Loss: 1.564757\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [39680/50000 (79%)] Loss: 1.576191\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [40320/50000 (81%)] Loss: 1.719374\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [40960/50000 (82%)] Loss: 1.782607\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [41600/50000 (83%)] Loss: 1.749955\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [42240/50000 (84%)] Loss: 1.791288\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [42880/50000 (86%)] Loss: 1.942103\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [43520/50000 (87%)] Loss: 1.808065\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [44160/50000 (88%)] Loss: 1.533260\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [44800/50000 (90%)] Loss: 1.853266\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [45440/50000 (91%)] Loss: 1.840446\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [46080/50000 (92%)] Loss: 1.821648\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [46720/50000 (93%)] Loss: 1.714493\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [47360/50000 (95%)] Loss: 1.790465\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [48000/50000 (96%)] Loss: 1.508352\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [48640/50000 (97%)] Loss: 1.581581\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [49280/50000 (98%)] Loss: 1.694627\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 10 [49920/50000 (100%)] Loss: 1.609440\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.3938, Accuracy: 55.15%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [640/50000 (1%)] Loss: 1.651727\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [1280/50000 (3%)] Loss: 1.614410\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [1920/50000 (4%)] Loss: 1.859875\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [2560/50000 (5%)] Loss: 1.733831\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [3200/50000 (6%)] Loss: 1.885736\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [3840/50000 (8%)] Loss: 1.879559\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [4480/50000 (9%)] Loss: 1.510721\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [5120/50000 (10%)] Loss: 1.819339\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [5760/50000 (12%)] Loss: 1.765088\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [6400/50000 (13%)] Loss: 1.516061\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [7040/50000 (14%)] Loss: 1.703189\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [7680/50000 (15%)] Loss: 1.760126\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [8320/50000 (17%)] Loss: 1.720488\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [8960/50000 (18%)] Loss: 1.731060\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [9600/50000 (19%)] Loss: 1.629425\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [10240/50000 (20%)] Loss: 1.460000\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [10880/50000 (22%)] Loss: 1.610173\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [11520/50000 (23%)] Loss: 1.588236\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [12160/50000 (24%)] Loss: 1.782977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [12800/50000 (26%)] Loss: 1.583859\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [13440/50000 (27%)] Loss: 1.700031\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [14080/50000 (28%)] Loss: 1.925434\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [14720/50000 (29%)] Loss: 1.713016\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [15360/50000 (31%)] Loss: 1.915172\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [16000/50000 (32%)] Loss: 1.920422\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [16640/50000 (33%)] Loss: 1.600315\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [17280/50000 (35%)] Loss: 1.605446\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [17920/50000 (36%)] Loss: 1.534023\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [18560/50000 (37%)] Loss: 1.689922\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [19200/50000 (38%)] Loss: 1.878864\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [19840/50000 (40%)] Loss: 1.748187\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [20480/50000 (41%)] Loss: 1.601366\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [21120/50000 (42%)] Loss: 1.729861\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [21760/50000 (43%)] Loss: 1.510872\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [22400/50000 (45%)] Loss: 1.826592\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [23040/50000 (46%)] Loss: 1.841988\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [23680/50000 (47%)] Loss: 1.819360\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [24320/50000 (49%)] Loss: 1.835594\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [24960/50000 (50%)] Loss: 1.737749\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [25600/50000 (51%)] Loss: 1.709599\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [26240/50000 (52%)] Loss: 1.754403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [26880/50000 (54%)] Loss: 1.641499\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [27520/50000 (55%)] Loss: 1.568907\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [28160/50000 (56%)] Loss: 1.735593\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [28800/50000 (58%)] Loss: 1.705427\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [29440/50000 (59%)] Loss: 1.727529\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [30080/50000 (60%)] Loss: 1.559458\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [30720/50000 (61%)] Loss: 1.750827\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [31360/50000 (63%)] Loss: 1.608302\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [32000/50000 (64%)] Loss: 1.561825\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [32640/50000 (65%)] Loss: 1.705127\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [33280/50000 (66%)] Loss: 1.555902\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [33920/50000 (68%)] Loss: 1.638991\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [34560/50000 (69%)] Loss: 1.797078\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [35200/50000 (70%)] Loss: 1.707914\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [35840/50000 (72%)] Loss: 1.684618\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [36480/50000 (73%)] Loss: 1.680884\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [37120/50000 (74%)] Loss: 1.770221\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [37760/50000 (75%)] Loss: 1.524855\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [38400/50000 (77%)] Loss: 1.611446\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [39040/50000 (78%)] Loss: 1.651789\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [39680/50000 (79%)] Loss: 1.714257\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [40320/50000 (81%)] Loss: 1.609176\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [40960/50000 (82%)] Loss: 1.626970\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [41600/50000 (83%)] Loss: 1.516145\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [42240/50000 (84%)] Loss: 1.756415\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [42880/50000 (86%)] Loss: 1.753201\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [43520/50000 (87%)] Loss: 1.619622\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [44160/50000 (88%)] Loss: 1.528055\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [44800/50000 (90%)] Loss: 1.568998\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [45440/50000 (91%)] Loss: 1.936599\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [46080/50000 (92%)] Loss: 1.778903\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [46720/50000 (93%)] Loss: 1.588859\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [47360/50000 (95%)] Loss: 1.786227\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [48000/50000 (96%)] Loss: 1.726332\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [48640/50000 (97%)] Loss: 1.643828\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [49280/50000 (98%)] Loss: 1.498221\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 11 [49920/50000 (100%)] Loss: 1.819983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.3865, Accuracy: 56.72%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [640/50000 (1%)] Loss: 1.488280\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [1280/50000 (3%)] Loss: 1.611758\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [1920/50000 (4%)] Loss: 1.710245\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [2560/50000 (5%)] Loss: 1.680940\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [3200/50000 (6%)] Loss: 1.730313\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [3840/50000 (8%)] Loss: 1.635078\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [4480/50000 (9%)] Loss: 1.554018\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [5120/50000 (10%)] Loss: 1.708944\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [5760/50000 (12%)] Loss: 1.831002\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [6400/50000 (13%)] Loss: 1.536215\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [7040/50000 (14%)] Loss: 1.609140\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [7680/50000 (15%)] Loss: 1.740070\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [8320/50000 (17%)] Loss: 1.671508\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [8960/50000 (18%)] Loss: 1.697834\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [9600/50000 (19%)] Loss: 1.650697\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [10240/50000 (20%)] Loss: 1.493908\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [10880/50000 (22%)] Loss: 1.575756\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [11520/50000 (23%)] Loss: 1.435695\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [12160/50000 (24%)] Loss: 1.458177\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [12800/50000 (26%)] Loss: 1.624106\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [13440/50000 (27%)] Loss: 1.768571\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [14080/50000 (28%)] Loss: 1.790736\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [14720/50000 (29%)] Loss: 1.655706\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [15360/50000 (31%)] Loss: 1.758106\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [16000/50000 (32%)] Loss: 1.770258\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [16640/50000 (33%)] Loss: 1.705214\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [17280/50000 (35%)] Loss: 1.631983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [17920/50000 (36%)] Loss: 1.689679\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [18560/50000 (37%)] Loss: 1.561055\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [19200/50000 (38%)] Loss: 1.780773\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [19840/50000 (40%)] Loss: 1.616087\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [20480/50000 (41%)] Loss: 1.761472\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [21120/50000 (42%)] Loss: 1.766272\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [21760/50000 (43%)] Loss: 1.441287\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [22400/50000 (45%)] Loss: 1.795744\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [23040/50000 (46%)] Loss: 1.897638\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [23680/50000 (47%)] Loss: 1.699345\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [24320/50000 (49%)] Loss: 1.672689\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [24960/50000 (50%)] Loss: 1.867852\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [25600/50000 (51%)] Loss: 1.614133\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [26240/50000 (52%)] Loss: 1.826058\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [26880/50000 (54%)] Loss: 1.631250\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [27520/50000 (55%)] Loss: 1.538947\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [28160/50000 (56%)] Loss: 1.553657\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [28800/50000 (58%)] Loss: 1.712908\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [29440/50000 (59%)] Loss: 1.787599\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [30080/50000 (60%)] Loss: 1.497130\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [30720/50000 (61%)] Loss: 1.712075\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [31360/50000 (63%)] Loss: 1.447067\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [32000/50000 (64%)] Loss: 1.774577\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [32640/50000 (65%)] Loss: 1.688860\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [33280/50000 (66%)] Loss: 1.672637\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [33920/50000 (68%)] Loss: 1.569282\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [34560/50000 (69%)] Loss: 1.703200\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [35200/50000 (70%)] Loss: 1.660174\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [35840/50000 (72%)] Loss: 1.570094\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [36480/50000 (73%)] Loss: 1.700176\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [37120/50000 (74%)] Loss: 1.675304\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [37760/50000 (75%)] Loss: 1.527868\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [38400/50000 (77%)] Loss: 1.811097\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [39040/50000 (78%)] Loss: 1.436822\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [39680/50000 (79%)] Loss: 1.627921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [40320/50000 (81%)] Loss: 1.651908\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [40960/50000 (82%)] Loss: 1.680725\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [41600/50000 (83%)] Loss: 1.558097\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [42240/50000 (84%)] Loss: 1.753627\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [42880/50000 (86%)] Loss: 1.622256\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [43520/50000 (87%)] Loss: 1.713714\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [44160/50000 (88%)] Loss: 1.415474\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [44800/50000 (90%)] Loss: 1.571508\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [45440/50000 (91%)] Loss: 1.818707\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [46080/50000 (92%)] Loss: 1.625340\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [46720/50000 (93%)] Loss: 1.605708\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [47360/50000 (95%)] Loss: 1.619861\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [48000/50000 (96%)] Loss: 1.750117\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [48640/50000 (97%)] Loss: 1.428399\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [49280/50000 (98%)] Loss: 1.614611\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 12 [49920/50000 (100%)] Loss: 1.581863\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.3506, Accuracy: 56.86%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [640/50000 (1%)] Loss: 1.415402\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [1280/50000 (3%)] Loss: 1.615270\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [1920/50000 (4%)] Loss: 1.489472\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [2560/50000 (5%)] Loss: 1.649620\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [3200/50000 (6%)] Loss: 1.761205\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [3840/50000 (8%)] Loss: 1.622851\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [4480/50000 (9%)] Loss: 1.264224\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [5120/50000 (10%)] Loss: 1.473527\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [5760/50000 (12%)] Loss: 1.743977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [6400/50000 (13%)] Loss: 1.810629\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [7040/50000 (14%)] Loss: 1.618159\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [7680/50000 (15%)] Loss: 1.904494\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [8320/50000 (17%)] Loss: 1.705722\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [8960/50000 (18%)] Loss: 1.784639\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [9600/50000 (19%)] Loss: 1.742799\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [10240/50000 (20%)] Loss: 1.575293\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [10880/50000 (22%)] Loss: 1.712353\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [11520/50000 (23%)] Loss: 1.552093\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [12160/50000 (24%)] Loss: 1.606204\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [12800/50000 (26%)] Loss: 1.786594\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [13440/50000 (27%)] Loss: 1.690918\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [14080/50000 (28%)] Loss: 1.807553\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [14720/50000 (29%)] Loss: 1.518983\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [15360/50000 (31%)] Loss: 1.746786\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [16000/50000 (32%)] Loss: 1.691911\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [16640/50000 (33%)] Loss: 1.535085\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [17280/50000 (35%)] Loss: 1.668591\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [17920/50000 (36%)] Loss: 1.779568\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [18560/50000 (37%)] Loss: 1.658221\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [19200/50000 (38%)] Loss: 1.753803\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [19840/50000 (40%)] Loss: 1.688251\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [20480/50000 (41%)] Loss: 1.465919\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [21120/50000 (42%)] Loss: 1.829135\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [21760/50000 (43%)] Loss: 1.428519\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [22400/50000 (45%)] Loss: 1.772151\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [23040/50000 (46%)] Loss: 1.814820\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [23680/50000 (47%)] Loss: 1.699468\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [24320/50000 (49%)] Loss: 1.721652\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [24960/50000 (50%)] Loss: 1.609375\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [25600/50000 (51%)] Loss: 1.683556\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [26240/50000 (52%)] Loss: 1.598954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [26880/50000 (54%)] Loss: 1.428079\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [27520/50000 (55%)] Loss: 1.676433\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [28160/50000 (56%)] Loss: 1.679086\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [28800/50000 (58%)] Loss: 1.651142\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [29440/50000 (59%)] Loss: 1.643406\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [30080/50000 (60%)] Loss: 1.550617\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [30720/50000 (61%)] Loss: 1.628659\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [31360/50000 (63%)] Loss: 1.450819\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [32000/50000 (64%)] Loss: 1.652705\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [32640/50000 (65%)] Loss: 1.835790\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [33280/50000 (66%)] Loss: 1.782098\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [33920/50000 (68%)] Loss: 1.587806\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [34560/50000 (69%)] Loss: 1.752086\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [35200/50000 (70%)] Loss: 1.625035\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [35840/50000 (72%)] Loss: 1.683708\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [36480/50000 (73%)] Loss: 1.554545\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [37120/50000 (74%)] Loss: 1.654976\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [37760/50000 (75%)] Loss: 1.612142\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [38400/50000 (77%)] Loss: 1.575082\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [39040/50000 (78%)] Loss: 1.396193\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [39680/50000 (79%)] Loss: 1.543720\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [40320/50000 (81%)] Loss: 1.630022\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [40960/50000 (82%)] Loss: 1.716881\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [41600/50000 (83%)] Loss: 1.709668\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [42240/50000 (84%)] Loss: 1.699417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [42880/50000 (86%)] Loss: 1.628410\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [43520/50000 (87%)] Loss: 1.696996\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [44160/50000 (88%)] Loss: 1.568442\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [44800/50000 (90%)] Loss: 1.538805\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [45440/50000 (91%)] Loss: 1.681104\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [46080/50000 (92%)] Loss: 1.614709\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [46720/50000 (93%)] Loss: 1.518586\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [47360/50000 (95%)] Loss: 1.649063\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [48000/50000 (96%)] Loss: 1.755562\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [48640/50000 (97%)] Loss: 1.616925\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [49280/50000 (98%)] Loss: 1.590212\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 13 [49920/50000 (100%)] Loss: 1.748001\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.3325, Accuracy: 55.92%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [640/50000 (1%)] Loss: 1.381966\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [1280/50000 (3%)] Loss: 1.619021\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [1920/50000 (4%)] Loss: 1.684966\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [2560/50000 (5%)] Loss: 1.750877\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [3200/50000 (6%)] Loss: 1.686227\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [3840/50000 (8%)] Loss: 1.529090\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [4480/50000 (9%)] Loss: 1.480459\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [5120/50000 (10%)] Loss: 1.693501\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [5760/50000 (12%)] Loss: 1.812136\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [6400/50000 (13%)] Loss: 1.606838\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [7040/50000 (14%)] Loss: 1.561230\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [7680/50000 (15%)] Loss: 1.723976\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [8320/50000 (17%)] Loss: 1.808429\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [8960/50000 (18%)] Loss: 1.826006\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [9600/50000 (19%)] Loss: 1.643036\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [10240/50000 (20%)] Loss: 1.490581\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [10880/50000 (22%)] Loss: 1.589213\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [11520/50000 (23%)] Loss: 1.715699\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [12160/50000 (24%)] Loss: 1.693250\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [12800/50000 (26%)] Loss: 1.645441\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [13440/50000 (27%)] Loss: 1.614371\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [14080/50000 (28%)] Loss: 1.700805\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [14720/50000 (29%)] Loss: 1.483590\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [15360/50000 (31%)] Loss: 1.629563\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [16000/50000 (32%)] Loss: 1.725310\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [16640/50000 (33%)] Loss: 1.598586\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [17280/50000 (35%)] Loss: 1.717829\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [17920/50000 (36%)] Loss: 2.005475\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [18560/50000 (37%)] Loss: 1.756604\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [19200/50000 (38%)] Loss: 1.813156\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [19840/50000 (40%)] Loss: 1.648954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [20480/50000 (41%)] Loss: 1.795450\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [21120/50000 (42%)] Loss: 1.779560\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [21760/50000 (43%)] Loss: 1.299417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [22400/50000 (45%)] Loss: 1.730698\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [23040/50000 (46%)] Loss: 1.788971\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [23680/50000 (47%)] Loss: 1.840778\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [24320/50000 (49%)] Loss: 1.730802\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [24960/50000 (50%)] Loss: 1.716490\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [25600/50000 (51%)] Loss: 1.534255\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [26240/50000 (52%)] Loss: 1.526044\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [26880/50000 (54%)] Loss: 1.638880\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [27520/50000 (55%)] Loss: 1.639495\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [28160/50000 (56%)] Loss: 1.729732\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [28800/50000 (58%)] Loss: 1.564658\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [29440/50000 (59%)] Loss: 1.656689\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [30080/50000 (60%)] Loss: 1.466985\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [30720/50000 (61%)] Loss: 1.572707\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [31360/50000 (63%)] Loss: 1.600144\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [32000/50000 (64%)] Loss: 1.532503\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [32640/50000 (65%)] Loss: 1.600278\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [33280/50000 (66%)] Loss: 1.674567\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [33920/50000 (68%)] Loss: 1.532328\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [34560/50000 (69%)] Loss: 1.658180\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [35200/50000 (70%)] Loss: 1.577858\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [35840/50000 (72%)] Loss: 1.658632\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [36480/50000 (73%)] Loss: 1.670305\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [37120/50000 (74%)] Loss: 1.709975\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [37760/50000 (75%)] Loss: 1.593559\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [38400/50000 (77%)] Loss: 1.586679\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [39040/50000 (78%)] Loss: 1.559553\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [39680/50000 (79%)] Loss: 1.599207\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [40320/50000 (81%)] Loss: 1.634490\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [40960/50000 (82%)] Loss: 1.677414\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [41600/50000 (83%)] Loss: 1.748754\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [42240/50000 (84%)] Loss: 1.666839\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [42880/50000 (86%)] Loss: 1.495192\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [43520/50000 (87%)] Loss: 1.569672\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [44160/50000 (88%)] Loss: 1.567542\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [44800/50000 (90%)] Loss: 1.540978\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [45440/50000 (91%)] Loss: 1.776803\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [46080/50000 (92%)] Loss: 1.396542\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [46720/50000 (93%)] Loss: 1.618419\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [47360/50000 (95%)] Loss: 1.590893\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [48000/50000 (96%)] Loss: 1.559928\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [48640/50000 (97%)] Loss: 1.568345\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [49280/50000 (98%)] Loss: 1.655072\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 14 [49920/50000 (100%)] Loss: 1.600250\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.2771, Accuracy: 58.29%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [640/50000 (1%)] Loss: 1.658198\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [1280/50000 (3%)] Loss: 1.385142\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [1920/50000 (4%)] Loss: 1.578566\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [2560/50000 (5%)] Loss: 1.678732\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [3200/50000 (6%)] Loss: 1.454901\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [3840/50000 (8%)] Loss: 1.684185\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [4480/50000 (9%)] Loss: 1.434085\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [5120/50000 (10%)] Loss: 1.716067\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [5760/50000 (12%)] Loss: 1.872890\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [6400/50000 (13%)] Loss: 1.580793\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [7040/50000 (14%)] Loss: 1.584882\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [7680/50000 (15%)] Loss: 1.785590\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [8320/50000 (17%)] Loss: 1.802484\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [8960/50000 (18%)] Loss: 1.766886\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [9600/50000 (19%)] Loss: 1.756803\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [10240/50000 (20%)] Loss: 1.561399\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [10880/50000 (22%)] Loss: 1.471242\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [11520/50000 (23%)] Loss: 1.666888\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [12160/50000 (24%)] Loss: 1.709935\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [12800/50000 (26%)] Loss: 1.469635\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [13440/50000 (27%)] Loss: 1.680992\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [14080/50000 (28%)] Loss: 1.576716\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [14720/50000 (29%)] Loss: 1.587713\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [15360/50000 (31%)] Loss: 1.581071\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [16000/50000 (32%)] Loss: 1.828713\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [16640/50000 (33%)] Loss: 1.623845\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [17280/50000 (35%)] Loss: 1.574190\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [17920/50000 (36%)] Loss: 1.552470\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [18560/50000 (37%)] Loss: 1.779784\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [19200/50000 (38%)] Loss: 1.805647\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [19840/50000 (40%)] Loss: 1.474818\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [20480/50000 (41%)] Loss: 1.567381\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [21120/50000 (42%)] Loss: 1.784783\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [21760/50000 (43%)] Loss: 1.414526\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [22400/50000 (45%)] Loss: 1.605583\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [23040/50000 (46%)] Loss: 1.599548\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [23680/50000 (47%)] Loss: 1.860938\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [24320/50000 (49%)] Loss: 1.598964\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [24960/50000 (50%)] Loss: 1.853089\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [25600/50000 (51%)] Loss: 1.524668\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [26240/50000 (52%)] Loss: 1.509456\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [26880/50000 (54%)] Loss: 1.622373\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [27520/50000 (55%)] Loss: 1.594194\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [28160/50000 (56%)] Loss: 1.675767\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [28800/50000 (58%)] Loss: 1.642531\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [29440/50000 (59%)] Loss: 1.683401\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [30080/50000 (60%)] Loss: 1.498655\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [30720/50000 (61%)] Loss: 1.896283\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [31360/50000 (63%)] Loss: 1.265671\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [32000/50000 (64%)] Loss: 1.677526\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [32640/50000 (65%)] Loss: 1.627537\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [33280/50000 (66%)] Loss: 1.697936\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [33920/50000 (68%)] Loss: 1.742952\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [34560/50000 (69%)] Loss: 1.502373\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [35200/50000 (70%)] Loss: 1.619867\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [35840/50000 (72%)] Loss: 1.507539\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [36480/50000 (73%)] Loss: 1.592414\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [37120/50000 (74%)] Loss: 1.694460\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [37760/50000 (75%)] Loss: 1.573444\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [38400/50000 (77%)] Loss: 1.533954\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [39040/50000 (78%)] Loss: 1.476302\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [39680/50000 (79%)] Loss: 1.727979\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [40320/50000 (81%)] Loss: 1.589663\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [40960/50000 (82%)] Loss: 1.651821\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [41600/50000 (83%)] Loss: 1.618730\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [42240/50000 (84%)] Loss: 1.518862\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [42880/50000 (86%)] Loss: 1.559036\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [43520/50000 (87%)] Loss: 1.478853\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [44160/50000 (88%)] Loss: 1.590941\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [44800/50000 (90%)] Loss: 1.700105\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [45440/50000 (91%)] Loss: 1.727403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [46080/50000 (92%)] Loss: 1.568329\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [46720/50000 (93%)] Loss: 1.629572\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [47360/50000 (95%)] Loss: 1.551406\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [48000/50000 (96%)] Loss: 1.644603\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [48640/50000 (97%)] Loss: 1.552173\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [49280/50000 (98%)] Loss: 1.688952\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 15 [49920/50000 (100%)] Loss: 1.662103\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.2892, Accuracy: 58.75%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [640/50000 (1%)] Loss: 1.469808\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [1280/50000 (3%)] Loss: 1.458519\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [1920/50000 (4%)] Loss: 1.509182\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [2560/50000 (5%)] Loss: 1.559977\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [3200/50000 (6%)] Loss: 1.658337\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [3840/50000 (8%)] Loss: 1.651108\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [4480/50000 (9%)] Loss: 1.375121\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [5120/50000 (10%)] Loss: 1.446072\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [5760/50000 (12%)] Loss: 1.902956\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [6400/50000 (13%)] Loss: 1.506124\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [7040/50000 (14%)] Loss: 1.685089\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [7680/50000 (15%)] Loss: 1.682913\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [8320/50000 (17%)] Loss: 1.505996\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [8960/50000 (18%)] Loss: 1.757427\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [9600/50000 (19%)] Loss: 1.768908\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [10240/50000 (20%)] Loss: 1.450666\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [10880/50000 (22%)] Loss: 1.761872\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [11520/50000 (23%)] Loss: 1.442369\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [12160/50000 (24%)] Loss: 1.651389\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [12800/50000 (26%)] Loss: 1.469034\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [13440/50000 (27%)] Loss: 1.732380\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [14080/50000 (28%)] Loss: 1.818897\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [14720/50000 (29%)] Loss: 1.489946\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [15360/50000 (31%)] Loss: 1.787068\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [16000/50000 (32%)] Loss: 1.608098\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [16640/50000 (33%)] Loss: 1.700144\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [17280/50000 (35%)] Loss: 1.645256\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [17920/50000 (36%)] Loss: 1.722699\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [18560/50000 (37%)] Loss: 1.631614\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [19200/50000 (38%)] Loss: 1.737764\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [19840/50000 (40%)] Loss: 1.662404\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [20480/50000 (41%)] Loss: 1.627139\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [21120/50000 (42%)] Loss: 1.938146\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [21760/50000 (43%)] Loss: 1.535496\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [22400/50000 (45%)] Loss: 1.785614\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [23040/50000 (46%)] Loss: 1.567631\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [23680/50000 (47%)] Loss: 1.566723\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [24320/50000 (49%)] Loss: 1.485360\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [24960/50000 (50%)] Loss: 1.743724\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [25600/50000 (51%)] Loss: 1.323995\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [26240/50000 (52%)] Loss: 1.709038\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [26880/50000 (54%)] Loss: 1.540813\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [27520/50000 (55%)] Loss: 1.572291\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [28160/50000 (56%)] Loss: 1.573736\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [28800/50000 (58%)] Loss: 1.630325\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [29440/50000 (59%)] Loss: 1.400094\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [30080/50000 (60%)] Loss: 1.538800\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [30720/50000 (61%)] Loss: 1.725911\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [31360/50000 (63%)] Loss: 1.278281\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [32000/50000 (64%)] Loss: 1.765093\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [32640/50000 (65%)] Loss: 1.759779\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [33280/50000 (66%)] Loss: 1.569403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [33920/50000 (68%)] Loss: 1.593090\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [34560/50000 (69%)] Loss: 1.445528\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [35200/50000 (70%)] Loss: 1.601601\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [35840/50000 (72%)] Loss: 1.567317\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [36480/50000 (73%)] Loss: 1.624883\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [37120/50000 (74%)] Loss: 1.588520\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [37760/50000 (75%)] Loss: 1.599293\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [38400/50000 (77%)] Loss: 1.615417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [39040/50000 (78%)] Loss: 1.710318\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [39680/50000 (79%)] Loss: 1.586800\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [40320/50000 (81%)] Loss: 1.702199\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [40960/50000 (82%)] Loss: 1.670758\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [41600/50000 (83%)] Loss: 1.599838\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [42240/50000 (84%)] Loss: 1.586625\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [42880/50000 (86%)] Loss: 1.538493\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [43520/50000 (87%)] Loss: 1.639440\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [44160/50000 (88%)] Loss: 1.434849\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [44800/50000 (90%)] Loss: 1.549702\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [45440/50000 (91%)] Loss: 1.598207\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [46080/50000 (92%)] Loss: 1.621249\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [46720/50000 (93%)] Loss: 1.775566\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [47360/50000 (95%)] Loss: 1.821650\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [48000/50000 (96%)] Loss: 1.394603\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [48640/50000 (97%)] Loss: 1.671963\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [49280/50000 (98%)] Loss: 1.411860\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 16 [49920/50000 (100%)] Loss: 1.587362\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.2767, Accuracy: 58.20%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [640/50000 (1%)] Loss: 1.496650\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [1280/50000 (3%)] Loss: 1.676779\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [1920/50000 (4%)] Loss: 1.527015\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [2560/50000 (5%)] Loss: 1.665705\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [3200/50000 (6%)] Loss: 1.712240\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [3840/50000 (8%)] Loss: 1.624534\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [4480/50000 (9%)] Loss: 1.381322\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [5120/50000 (10%)] Loss: 1.570705\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [5760/50000 (12%)] Loss: 1.744766\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [6400/50000 (13%)] Loss: 1.528876\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [7040/50000 (14%)] Loss: 1.762938\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [7680/50000 (15%)] Loss: 1.679053\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [8320/50000 (17%)] Loss: 1.698523\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [8960/50000 (18%)] Loss: 1.659038\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [9600/50000 (19%)] Loss: 1.763014\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [10240/50000 (20%)] Loss: 1.401070\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [10880/50000 (22%)] Loss: 1.562496\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [11520/50000 (23%)] Loss: 1.651790\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [12160/50000 (24%)] Loss: 1.604637\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [12800/50000 (26%)] Loss: 1.523825\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [13440/50000 (27%)] Loss: 1.583022\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [14080/50000 (28%)] Loss: 1.682882\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [14720/50000 (29%)] Loss: 1.674139\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [15360/50000 (31%)] Loss: 1.636159\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [16000/50000 (32%)] Loss: 1.590428\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [16640/50000 (33%)] Loss: 1.583559\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [17280/50000 (35%)] Loss: 1.530537\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [17920/50000 (36%)] Loss: 1.645581\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [18560/50000 (37%)] Loss: 1.890518\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [19200/50000 (38%)] Loss: 1.638792\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [19840/50000 (40%)] Loss: 1.715585\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [20480/50000 (41%)] Loss: 1.610776\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [21120/50000 (42%)] Loss: 1.745628\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [21760/50000 (43%)] Loss: 1.160862\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [22400/50000 (45%)] Loss: 1.546446\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [23040/50000 (46%)] Loss: 1.688658\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [23680/50000 (47%)] Loss: 1.639417\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [24320/50000 (49%)] Loss: 1.500474\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [24960/50000 (50%)] Loss: 1.825722\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [25600/50000 (51%)] Loss: 1.607750\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [26240/50000 (52%)] Loss: 1.610427\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [26880/50000 (54%)] Loss: 1.414109\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [27520/50000 (55%)] Loss: 1.668218\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [28160/50000 (56%)] Loss: 1.595804\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [28800/50000 (58%)] Loss: 1.784468\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [29440/50000 (59%)] Loss: 1.429245\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [30080/50000 (60%)] Loss: 1.514284\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [30720/50000 (61%)] Loss: 1.674008\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [31360/50000 (63%)] Loss: 1.413267\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [32000/50000 (64%)] Loss: 1.656646\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [32640/50000 (65%)] Loss: 1.483424\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [33280/50000 (66%)] Loss: 1.506167\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [33920/50000 (68%)] Loss: 1.490244\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [34560/50000 (69%)] Loss: 1.514590\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [35200/50000 (70%)] Loss: 1.591805\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [35840/50000 (72%)] Loss: 1.391177\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [36480/50000 (73%)] Loss: 1.529909\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [37120/50000 (74%)] Loss: 1.500652\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [37760/50000 (75%)] Loss: 1.758279\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [38400/50000 (77%)] Loss: 1.383070\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [39040/50000 (78%)] Loss: 1.584603\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [39680/50000 (79%)] Loss: 1.638334\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [40320/50000 (81%)] Loss: 1.678278\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [40960/50000 (82%)] Loss: 1.782608\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [41600/50000 (83%)] Loss: 1.540434\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [42240/50000 (84%)] Loss: 1.595172\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [42880/50000 (86%)] Loss: 1.627454\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [43520/50000 (87%)] Loss: 1.464885\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [44160/50000 (88%)] Loss: 1.367187\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [44800/50000 (90%)] Loss: 1.488689\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [45440/50000 (91%)] Loss: 1.602911\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [46080/50000 (92%)] Loss: 1.578855\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [46720/50000 (93%)] Loss: 1.490147\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [47360/50000 (95%)] Loss: 1.618871\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [48000/50000 (96%)] Loss: 1.445523\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [48640/50000 (97%)] Loss: 1.592814\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [49280/50000 (98%)] Loss: 1.523751\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 17 [49920/50000 (100%)] Loss: 1.623072\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.2333, Accuracy: 60.25%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [640/50000 (1%)] Loss: 1.392874\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [1280/50000 (3%)] Loss: 1.592388\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [1920/50000 (4%)] Loss: 1.591467\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [2560/50000 (5%)] Loss: 1.635738\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [3200/50000 (6%)] Loss: 1.483077\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [3840/50000 (8%)] Loss: 1.635844\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [4480/50000 (9%)] Loss: 1.474059\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [5120/50000 (10%)] Loss: 1.387349\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [5760/50000 (12%)] Loss: 1.852615\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [6400/50000 (13%)] Loss: 1.463078\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [7040/50000 (14%)] Loss: 1.629362\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [7680/50000 (15%)] Loss: 1.831050\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [8320/50000 (17%)] Loss: 1.752021\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [8960/50000 (18%)] Loss: 1.687753\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [9600/50000 (19%)] Loss: 1.664166\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [10240/50000 (20%)] Loss: 1.487157\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [10880/50000 (22%)] Loss: 1.738596\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [11520/50000 (23%)] Loss: 1.664578\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [12160/50000 (24%)] Loss: 1.684561\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [12800/50000 (26%)] Loss: 1.500415\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [13440/50000 (27%)] Loss: 1.524645\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [14080/50000 (28%)] Loss: 1.720025\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [14720/50000 (29%)] Loss: 1.440867\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [15360/50000 (31%)] Loss: 1.549602\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [16000/50000 (32%)] Loss: 1.638912\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [16640/50000 (33%)] Loss: 1.508605\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [17280/50000 (35%)] Loss: 1.345580\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [17920/50000 (36%)] Loss: 1.528379\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [18560/50000 (37%)] Loss: 1.847609\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [19200/50000 (38%)] Loss: 1.594133\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [19840/50000 (40%)] Loss: 1.581265\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [20480/50000 (41%)] Loss: 1.538981\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [21120/50000 (42%)] Loss: 1.862291\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [21760/50000 (43%)] Loss: 1.397682\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [22400/50000 (45%)] Loss: 1.554049\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [23040/50000 (46%)] Loss: 1.653376\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [23680/50000 (47%)] Loss: 1.625260\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [24320/50000 (49%)] Loss: 1.657691\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [24960/50000 (50%)] Loss: 1.734123\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [25600/50000 (51%)] Loss: 1.501726\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [26240/50000 (52%)] Loss: 1.676356\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [26880/50000 (54%)] Loss: 1.505303\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [27520/50000 (55%)] Loss: 1.469421\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [28160/50000 (56%)] Loss: 1.555130\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [28800/50000 (58%)] Loss: 1.728792\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [29440/50000 (59%)] Loss: 1.531079\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [30080/50000 (60%)] Loss: 1.539204\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [30720/50000 (61%)] Loss: 1.585925\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [31360/50000 (63%)] Loss: 1.330127\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [32000/50000 (64%)] Loss: 1.396310\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [32640/50000 (65%)] Loss: 1.448162\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [33280/50000 (66%)] Loss: 1.448620\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [33920/50000 (68%)] Loss: 1.512392\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [34560/50000 (69%)] Loss: 1.609513\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [35200/50000 (70%)] Loss: 1.473132\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [35840/50000 (72%)] Loss: 1.472463\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [36480/50000 (73%)] Loss: 1.568350\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [37120/50000 (74%)] Loss: 1.674311\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [37760/50000 (75%)] Loss: 1.497585\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [38400/50000 (77%)] Loss: 1.401616\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [39040/50000 (78%)] Loss: 1.414238\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [39680/50000 (79%)] Loss: 1.572057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [40320/50000 (81%)] Loss: 1.637403\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [40960/50000 (82%)] Loss: 1.609139\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [41600/50000 (83%)] Loss: 1.610260\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [42240/50000 (84%)] Loss: 1.328769\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [42880/50000 (86%)] Loss: 1.405869\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [43520/50000 (87%)] Loss: 1.669934\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [44160/50000 (88%)] Loss: 1.340535\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [44800/50000 (90%)] Loss: 1.600563\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [45440/50000 (91%)] Loss: 1.704762\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [46080/50000 (92%)] Loss: 1.581997\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [46720/50000 (93%)] Loss: 1.657921\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [47360/50000 (95%)] Loss: 1.692539\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [48000/50000 (96%)] Loss: 1.404348\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [48640/50000 (97%)] Loss: 1.506546\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [49280/50000 (98%)] Loss: 1.752791\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 18 [49920/50000 (100%)] Loss: 1.643512\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.2161, Accuracy: 60.45%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [640/50000 (1%)] Loss: 1.371889\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [1280/50000 (3%)] Loss: 1.305303\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [1920/50000 (4%)] Loss: 1.717098\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [2560/50000 (5%)] Loss: 1.747582\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [3200/50000 (6%)] Loss: 1.618425\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [3840/50000 (8%)] Loss: 1.612841\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [4480/50000 (9%)] Loss: 1.245865\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [5120/50000 (10%)] Loss: 1.430468\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [5760/50000 (12%)] Loss: 1.714895\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [6400/50000 (13%)] Loss: 1.524368\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [7040/50000 (14%)] Loss: 1.675463\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [7680/50000 (15%)] Loss: 1.547930\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [8320/50000 (17%)] Loss: 1.532902\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [8960/50000 (18%)] Loss: 1.590034\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [9600/50000 (19%)] Loss: 1.657171\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [10240/50000 (20%)] Loss: 1.442434\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [10880/50000 (22%)] Loss: 1.580928\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [11520/50000 (23%)] Loss: 1.486233\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [12160/50000 (24%)] Loss: 1.522804\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [12800/50000 (26%)] Loss: 1.562393\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [13440/50000 (27%)] Loss: 1.619964\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [14080/50000 (28%)] Loss: 1.532741\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [14720/50000 (29%)] Loss: 1.456913\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [15360/50000 (31%)] Loss: 1.503608\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [16000/50000 (32%)] Loss: 1.537148\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [16640/50000 (33%)] Loss: 1.694537\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [17280/50000 (35%)] Loss: 1.544074\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [17920/50000 (36%)] Loss: 1.673234\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [18560/50000 (37%)] Loss: 1.519371\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [19200/50000 (38%)] Loss: 1.684795\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [19840/50000 (40%)] Loss: 1.570050\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [20480/50000 (41%)] Loss: 1.509207\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [21120/50000 (42%)] Loss: 1.952057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [21760/50000 (43%)] Loss: 1.438803\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [22400/50000 (45%)] Loss: 1.581985\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [23040/50000 (46%)] Loss: 1.642768\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [23680/50000 (47%)] Loss: 1.490807\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [24320/50000 (49%)] Loss: 1.494353\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [24960/50000 (50%)] Loss: 1.615916\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [25600/50000 (51%)] Loss: 1.602308\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [26240/50000 (52%)] Loss: 1.464609\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [26880/50000 (54%)] Loss: 1.544915\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [27520/50000 (55%)] Loss: 1.493581\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [28160/50000 (56%)] Loss: 1.580996\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [28800/50000 (58%)] Loss: 1.558472\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [29440/50000 (59%)] Loss: 1.506003\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [30080/50000 (60%)] Loss: 1.455197\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [30720/50000 (61%)] Loss: 1.531793\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [31360/50000 (63%)] Loss: 1.286672\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [32000/50000 (64%)] Loss: 1.676236\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [32640/50000 (65%)] Loss: 1.606418\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [33280/50000 (66%)] Loss: 1.529366\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [33920/50000 (68%)] Loss: 1.435398\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [34560/50000 (69%)] Loss: 1.586684\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [35200/50000 (70%)] Loss: 1.514057\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [35840/50000 (72%)] Loss: 1.482820\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [36480/50000 (73%)] Loss: 1.745860\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [37120/50000 (74%)] Loss: 1.569314\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [37760/50000 (75%)] Loss: 1.434569\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [38400/50000 (77%)] Loss: 1.516654\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [39040/50000 (78%)] Loss: 1.317586\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [39680/50000 (79%)] Loss: 1.357679\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [40320/50000 (81%)] Loss: 1.450904\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [40960/50000 (82%)] Loss: 1.540535\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [41600/50000 (83%)] Loss: 1.460118\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [42240/50000 (84%)] Loss: 1.600665\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [42880/50000 (86%)] Loss: 1.500554\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [43520/50000 (87%)] Loss: 1.603866\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [44160/50000 (88%)] Loss: 1.303345\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [44800/50000 (90%)] Loss: 1.423843\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [45440/50000 (91%)] Loss: 1.571655\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [46080/50000 (92%)] Loss: 1.572415\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [46720/50000 (93%)] Loss: 1.441381\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [47360/50000 (95%)] Loss: 1.639019\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [48000/50000 (96%)] Loss: 1.705283\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [48640/50000 (97%)] Loss: 1.496914\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [49280/50000 (98%)] Loss: 1.592981\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 19 [49920/50000 (100%)] Loss: 1.583114\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.1930, Accuracy: 60.93%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [640/50000 (1%)] Loss: 1.383044\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [1280/50000 (3%)] Loss: 1.607512\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [1920/50000 (4%)] Loss: 1.601349\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [2560/50000 (5%)] Loss: 1.758006\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [3200/50000 (6%)] Loss: 1.632898\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [3840/50000 (8%)] Loss: 1.794228\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [4480/50000 (9%)] Loss: 1.251013\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [5120/50000 (10%)] Loss: 1.705129\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [5760/50000 (12%)] Loss: 1.586481\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [6400/50000 (13%)] Loss: 1.560472\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [7040/50000 (14%)] Loss: 1.519876\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [7680/50000 (15%)] Loss: 1.652809\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [8320/50000 (17%)] Loss: 1.514010\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [8960/50000 (18%)] Loss: 1.624197\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [9600/50000 (19%)] Loss: 1.601674\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [10240/50000 (20%)] Loss: 1.434675\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [10880/50000 (22%)] Loss: 1.431558\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [11520/50000 (23%)] Loss: 1.507451\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [12160/50000 (24%)] Loss: 1.516244\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [12800/50000 (26%)] Loss: 1.496251\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [13440/50000 (27%)] Loss: 1.445717\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [14080/50000 (28%)] Loss: 1.587907\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [14720/50000 (29%)] Loss: 1.314667\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [15360/50000 (31%)] Loss: 1.606073\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [16000/50000 (32%)] Loss: 1.584275\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [16640/50000 (33%)] Loss: 1.587082\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [17280/50000 (35%)] Loss: 1.383963\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [17920/50000 (36%)] Loss: 1.481375\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [18560/50000 (37%)] Loss: 1.688373\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [19200/50000 (38%)] Loss: 1.594173\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [19840/50000 (40%)] Loss: 1.705553\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [20480/50000 (41%)] Loss: 1.765387\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [21120/50000 (42%)] Loss: 1.807378\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [21760/50000 (43%)] Loss: 1.330769\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [22400/50000 (45%)] Loss: 1.833494\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [23040/50000 (46%)] Loss: 1.677012\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [23680/50000 (47%)] Loss: 1.763178\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [24320/50000 (49%)] Loss: 1.385170\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [24960/50000 (50%)] Loss: 1.657798\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [25600/50000 (51%)] Loss: 1.612090\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [26240/50000 (52%)] Loss: 1.314453\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [26880/50000 (54%)] Loss: 1.624740\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [27520/50000 (55%)] Loss: 1.411653\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [28160/50000 (56%)] Loss: 1.746684\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [28800/50000 (58%)] Loss: 1.717382\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [29440/50000 (59%)] Loss: 1.531809\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [30080/50000 (60%)] Loss: 1.507517\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [30720/50000 (61%)] Loss: 1.534650\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [31360/50000 (63%)] Loss: 1.420687\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [32000/50000 (64%)] Loss: 1.402924\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [32640/50000 (65%)] Loss: 1.646072\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [33280/50000 (66%)] Loss: 1.464390\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [33920/50000 (68%)] Loss: 1.470997\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [34560/50000 (69%)] Loss: 1.615587\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [35200/50000 (70%)] Loss: 1.502680\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [35840/50000 (72%)] Loss: 1.434805\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [36480/50000 (73%)] Loss: 1.398623\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [37120/50000 (74%)] Loss: 1.537720\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [37760/50000 (75%)] Loss: 1.424721\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [38400/50000 (77%)] Loss: 1.510842\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [39040/50000 (78%)] Loss: 1.488477\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [39680/50000 (79%)] Loss: 1.546677\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [40320/50000 (81%)] Loss: 1.467783\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [40960/50000 (82%)] Loss: 1.740841\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [41600/50000 (83%)] Loss: 1.467450\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [42240/50000 (84%)] Loss: 1.403282\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [42880/50000 (86%)] Loss: 1.537259\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [43520/50000 (87%)] Loss: 1.544108\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [44160/50000 (88%)] Loss: 1.235982\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [44800/50000 (90%)] Loss: 1.461284\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [45440/50000 (91%)] Loss: 1.624588\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [46080/50000 (92%)] Loss: 1.609440\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [46720/50000 (93%)] Loss: 1.613379\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [47360/50000 (95%)] Loss: 1.576725\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [48000/50000 (96%)] Loss: 1.645453\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [48640/50000 (97%)] Loss: 1.462666\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [49280/50000 (98%)] Loss: 1.366487\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Train Epoch: 20 [49920/50000 (100%)] Loss: 1.676006\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Test set: Average loss: 1.1822, Accuracy: 62.91%\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Training is finished\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m Saving the model.\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m /opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m \n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 |\u001b[0m 2021-06-08 04:40:38,634 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36md0my0entk3-algo-1-2e2l1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "# from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# cifar10_estimator = PyTorch(\n",
    "#     entry_point=\"train_horovod.py\",    \n",
    "#     source_dir='source',    \n",
    "#     base_job_name = job_name,\n",
    "#     role=role,\n",
    "#     framework_version='1.6.0',\n",
    "#     py_version='py3',\n",
    "#     train_instance_count=1,\n",
    "#     train_instance_type=instance_type,\n",
    "#     hyperparameters={\"epochs\": 3, \n",
    "#                      'lr': 0.01,\n",
    "#                      'batch-size': 64,\n",
    "#                      \"backend\": \"gloo\",                     \n",
    "#                     },    \n",
    "# )\n",
    "# cifar10_estimator.fit({\"training\" : inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horovod_artifact_path = cifar10_estimator.model_data\n",
    "# print(\"horovod_artifact_path: \", horovod_artifact_path)\n",
    "\n",
    "\n",
    "# %store horovod_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 ls {horovod_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
